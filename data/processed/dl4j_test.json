{"ids": ["PayzkYJi", "mwXvchG1", "tKx7YoHx", "rK9v3mpy", "T8k5LGeT", "xIklqeKr", "QKopVrkL", "Arckh7Im", "DZUqhIgi", "TdP2EoK2", "nvyO1pXs", "bifvyGAX", "u821WrPi", "rQH1daEv", "6SBk1hmI", "OXPqnuMM", "IA0No09k", "nwXkwUeO", "3Cp9NLvI", "y7SWLiwh", "7QuD0jji", "sDUFvb0j", "RU71Wirz", "D45JaEVE", "PEVWgEil", "vyKy7la8", "U61Pzlyv", "hR6cJy9q", "6047cweG", "QM0tQnQo", "PdkdVWig", "SGj606vZ", "9tmFdYHD", "0568VsVA", "d9vUbgHg", "bsHv6DbG", "1Ht2X1eh", "BQ7LZGSe", "8qFhVOUN", "414D3raF", "bNjX9hAe", "aD2ytqFi", "jp5mdSCB", "6jgwmIY4", "CAaAGbjA", "ctlBQzXv", "hSpsbKnA", "DypjVQcZ", "J5l8pfBU", "FYkntgLb", "FxI5l6qI", "edeVoWj2", "8bh1jm2z", "RphxUv5T", "g7ztLmc0", "CtNEkYnV", "v46oNSNA", "pm0pSCZe", "dlJZnHwB", "NTwufKqh", "Lm7iourC", "r8wjmRVq", "kmH5EZED", "pbriCxvN", "7rOnRfLm", "Mx94AbRI", "oOoyCwNA", "nmS1TAYM", "3eWmUASo", "wNdxfzqe", "0eN5EPQK", "m7fQKFzK", "HrxHxmaB", "tukH3MaO", "771Q5Sev", "7VTVumkR", "KOwcpIXz", "u9BWGRoS", "PgtPstCV", "nxOCHR9s", "e3p2yCon", "koJ1rkNx", "QRaBUYs3", "q9Kf4ngp", "yCHzU3Fn", "siLXJhSy", "0KSjrHIo", "jibs2wGd", "wOHfnwFL", "EzUgoQz9", "vnUAk9u7", "EN44H4N9", "PyBjTCjS", "nCQ4T6P8", "Of2hIIuY", "4KWcCIVQ", "aaTlxqZ0", "RCOm0BYs", "qRjuYT3Q", "voAv3RVT", "hXps1rYx", "GnwgSFiG", "V8PoZQod", "bNeUBx8k", "lEysLvxr", "blrhM2t7", "eXbRqfcK", "5daCWChH", "AKH0HVlM", "InRx119i", "8biPlUpb", "j1O8YQGo", "Vw6jycMe", "aXsZ2DdQ", "ttRRpHPG", "xPZzvsF3", "9i0yt4e6", "yKHrdHSP", "zfUaSM0o", "lv0XUm9B", "u7XfIvLc", "vOl6sRYK", "m1ubNb2N", "PFSB31O6", "TDMbY0Bi", "Yh6l1UC8", "OOcpKFP8", "3NP7VmDC", "OLECxux9", "em5Vrf5z", "KJ4ANv31", "W1SiN8AH", "CyXXYdmU", "jZyKSMP6", "viYnanGy", "zWYxdfWC", "KUf6KBoN", "UjKplJtk", "LIRULJfE", "SdVBIEEa", "uiQ7pwTH", "trZ1oWpY", "CiH0lXCG", "hTBFQiDu", "ArRbLxIL", "oF9mNOMy", "4G9ObGCw", "jvCF1ZtY", "fcMlEPND", "3h7ZnX8l", "rVEnxS9q", "8pQ3Xu3i", "TJvIMTch", "fjm8Pc0d", "1ED09Z4d", "dyMrkQrR", "iUtoIYmR", "uIFubdOl", "oLdCkFME", "tDE0oRU9", "s3jG6igo", "Fo9wQyL3", "Oao3yP64", "NpGRrKX0", "nCoov4bD", "r94tXNCG", "aivtUI5D", "qNLgSo2t", "x3MtlzDH", "J59IM9D4", "iAHmRNLR", "cNLvkW8x", "Ifp4mUl2", "aN4AVUmW", "45BlKhqs", "kQH7XxSf", "dOKBsNQ4", "ZB6LsWw4", "28OCE5DM", "aYDqMbJC", "uUklV7h8", "8a1919Ii", "acsCGYvj", "RLusZVEd", "2FA1pnj6", "UpZhoDrX", "SQkTaBgZ", "KPkAnA5m", "x8xHp9f9", "mxrW4YWY", "SJhgFl0c", "4zVQeELi", "RLUqaQim", "mN2wFgxi", "MhwPS0lZ", "D4HTXlnb", "z0HfGHhG", "l5Z3g70i", "JRDlh8z3", "OfufPWyw", "XN5V132K", "i1e7sM3T", "7KcILZo9", "4nAaohjf", "HQdFFU4Q", "W8RmZKtt", "CNaiVfL4", "nTJSJvPn", "VWmfJc7F", "2898LCAN", "My9ELcgS", "ufEEMZ6X", "5QuvzgMn", "Jzodz1ag", "qfTUkN3D", "GzTCskgc", "HkDZDu4O", "3hg6VCSk", "veN5dwHC", "s3qbSK4J", "t1p7ggAf", "AmUaiyQ5", "HJrqxVZt", "F85Yryu7", "dATgY8Nj", "ioL7lveN", "NRWM42Ie", "blLJMIcA", "fqZOflCU", "8tSWdvZU", "7P6m8TA5", "vcmwCZA6", "HCj83qdb", "pcURINjA", "MC4KC1Vt", "NRbFiQSG", "YbMysnxy", "rdESlavP", "F2yQzPI5", "Xf3l23MO", "e326gRZf", "1WyRFyiy", "e8NEvqrj", "zjTITr5l", "MhycjMVI", "hc585C3C", "5EY6WU6r", "vI8rZrqq", "pSw9feuC", "LyjqkgYh", "fzfaNgBW", "Q8IEGgbi", "y8sWmtKi", "GLQWzlnG", "2z4P7lxU", "AVS7yJVR", "xVft5pmK"], "dialog": [["i'm trying to run a machine learning model on aws emr and i'm getting the error  code  even though the model works in intellij. is there a good way for me to bugcheck this and make sure that i have the appropriate libraries?", "just to add, i'm importing  code  already even though it isn't used directly in my model.", "hard to tell what your issue is. can you please post a completely reproducible problem (dependencies, how you're running your commands, versions of software,..) on the forums?  link  thanks!"], ["let me train a new model in id and test it out once... if that fails too i will share the models and file an issue", "just post configurations you have, i'll do it faster", "i know where to look there", "link", "how big is corpus? is it sentences or documents?", "documents = set of sentences. i.e. text file.", "documents", "amazing, thanks", "how big is it? how many documents, and what's average size of single document in bytes?", "we have 3 different models  : 1 with 2 lakhs docs, 1 with 32k docs and last one with 476 docs... all have average 10-12 sentencesa", "lakhs <- what's that word means?", "id  mil", "aha, thanks."], ["alex could you please explain the meaning of iteration number in the basicrnnexample?", "suppose you have 3 examples: a,b,c in a datasetiterator.iterations(1): fit(datasetiterator) does training like abc.iterations(3): fit(datasetiterator) does training like aaabbbccc", "usually .iterations(1) is what you want", "that was an example contributed by the community fwiw", "thank you.", "so in that example, it is similar to the epoch parameter", "if all data is in one dataset object: then .iterations(x) and fitting for x epochs are identical", "in general, similar to epochs, but different order", "repeated order like that isn't usually a good idea", "i see.", "in what situation setting iterations > 1 makes sense?", "not many, to be honest. full batch/dataset learning (usually only possible on very small data sets), andmaybeif data loading is very costly", "i seeff"], ["hi, i am new here,trying to use deeplearning4j on android, i followed the doc about dependencies, although, i cannot port a simple code that i used (word2vec model), the issue, is  code  not found (i need  code )... using  code  and  code", "i am sorry to interrupt, it seems deeplearning4j-nlp was missing."], ["anyone can help see my question mentioned before? i set the iteration score listener. in id , i can get the \"info\" of the iteration scores. but in id , there is no such score info although i set the listener as well.", "nothing changed there", "the only thing it would be is your logging", "check the commit logs yourself if you want", "ok, i will check then."], ["have you got a code example which simply has working code for tsne plot on word2vec output? the document only has snippet", "code for?", "for single method call?", "and the tsnestandardexample.java does not plot", "i\u2019ve already pointed you to method you need", "like tsnestandardexample.java, except one that actually put up a plot", "there\u2019s nothing more needed from you", "so you don't have such an example code"], ["hey, i'm trying to import a model from keras, which seems to be working, but i don't really understand how to include the json and hdf file in the output jar. i put them in a resources folder that's a resource root and they are present at the root of the jar file, but i get a no file found exception"], ["hello, i am trying to use dl4j in eclipse for an eclipse application using osgi. when i run a junit test, it works perfectly! however, when i try to run this as an eclipse application, it throws  code . now, this also means it cannot load the nd4j backend. i took a quick look at the packages and i saw that org.nd4j.native-api and org.nd4j.native.linux-x86_64 (the platform that i am using for nd4j backend) share the same package (org.nd4j.nativeblas). i suspect this would be the cause of this problem, since osgi have an issue with split-packages. would anybody know how to help me on this problem?", "eclipse has problems with transitive dependencies among other things. can you share your pom file via !gist?", "here is the pom  link . since it is a plug-in project (and not a maven project), i use reficio to generate the right dependencies for the project. as you can see, i have dl4j-core and nd4j-native-backend, and i added sl4j-nop for the logging configuration of sl4j. i then added the dependencies to the target definition and the manifest.mf  link", "i mean \"p2-maven-plugin\" instead of \"reficio\", sorry", "could you put all this in the issue on github  issue ?", "done"], ["i need help. i train a wide_deep keras model,but can not be imported by deeplearning4f. deeplearning4j do not support multi inputs?", "it does. if you use computationgraph. you have to make sure you call the right import function.", "thank you very much\uff0cit really does", "when i use maven version-1.-beta6 ,it has an error but 1.-beta7 is ok ."], ["is there a book or a good tutorial for nd4j ? also is nd4j support optimization for sequence of gpu operations: for instance 1. for all rows of a given matrix m 2. multiply another matrix with the specified row 3. select a range of row of the result matrix or etc. these operations can be further optimized", "for instance we can prevent loading the matrix to the gpu.", "for google indexing purposes and preventing people re asking questions, i'd suggest moving some activity over to the forum  link  - regarding tutorials we mainly have the examples and some docs on some of the key concepts like workspaces/memory management, indexing,.. if you can be more specific (again maybe on the forum so you can bookmark my answers ) - we can drill in to that a bit"], ["hello.what's difference bewteen nd4j-native and nd4j-native-platform? why nd4j-native is test scoped?  code", "is anyone there who can tell which backend i should choose ?", "native platform included the native binaries for every supported platform. so it\u2019s the safe bet. if you need to optimize the size for some reason you can limit it to a single platform (like linux-x86_64-avx2). to do that include the non platform dependency twice in the pom, and on the second one add a <classifier> with the platform you want to use.", "ok i'll use native platform. thank you."], ["hey, sorry for interrupting you, but i got a quick question, wondering if someone can answer it... if i run the following line of code on a pretty large nd4j array (no dl4j involved) with tons of negative infinity values in it, a lot of messages saying \"number: -infinity\" get logged. is this intentional, and if so, why?  code", "probably just a bug, leaked debug messsage", "i\u2019ll check that right now", "yes, because i've wrote that one", ":)", "assumed so. thanks anyway, should i file a report or something?", "thanks, no need, i\u2019m already opening nd4j", "alright", "i mean - you\u2019ve already reported it", "yeah but you might prefer it if i report it somewhere so you can organize your to-do-list or whatever", "found it, line 238 in booleanindexing in case you haven't yet", ":p"], ["hello everyone, i have been facing this issue in docker - \"please ensure that you have an nd4j backend on your classpath\"", "its a scala project and the dependencies are as below,  code . any help would be greatly appreciated, its running locally", "ok so setting nd4j_dynamic_load_classpath made it work just fine"], ["hey guys, i have one last question around spark's word2vec implementation (newest version). is there anyway to update existing weights, like the standalone word2vec version? for example, if i start with a corpus of 1 gb, then add 500 more mbs at a later time, can it do so without overwriting the existing vocab and inmemorylookuptable? if not, is there a way to merge two lookup tables? (i know you can add vocab cache, but i don't see anything for the inmemorylookuptable).", "there are methods for that, but problem isn't really a merge of two tables, but merge of two huffman trees.", "ah, i didn't seen anything that did that in the word2vec implementation. is it apart of the nlp package?", "dl4j-spark-nlp", "thanks"], ["how to frame a network which reads paragraph and answer question from that?", "have you set up the examples and read trhough the code?", "start there  link", "i have done all these but this example doesn't statisfy my usecase.", "got held up how to proceed", "question answering systems are alotmore complicated than most of our examples... you're going to have to research the deep learning literature and build it yourself", "can you suggest  some steps to achieve question answering system from paragraph?", "agibsonccc: can you suggest some steps to achieve question answering system from paragraph?", "i\u2019d suggest to go to arxiv.org as first step", "your question doesn\u2019t have simple answer"], ["hi, i tried to run gloveexample in dl4j-examples. but it failed with unsupportedoperationexeption. has glove been unsupported now? this is gist of the error message  link .", "ok i found learninsequence(sequence, nextrandom, alpha, batchsequences) in sequencevectors class is not supported  link   link .", "this is known issue, and wip", "i\u2019m happy to hear that. waiting for it fixed."], ["hi", "i would like to use this code in java eclipse instead of  intellij.   link", "there's nothing stopping you learn m2eclipse", "we won't support you if something hits a wall though", "we just don't use it", "so we can't really help with trouble shooting weird edge cases", "beyond that follow traditional tutorials on the internet", "we aren't anything unique as long as you're using a build system", "thanks"], ["does anybody know anything about whether jcuda can be used with dl4j?"], ["i'm running into oom issues, crash dump:  code . what's weird is that the xmx is 2gb, javacpp current bytes is <8gb, but javacpp current physical is a whopping 37gb. is javacpp current bytes on a per-thread basis? (using  code , trying to see how high i can go) how much memory is used by each worker? reading the crash report, it looks like it should be something like network:  id gb, activations:  id gb, so <3gb total? haven't configured workspaces as they're on by default in 1.-beta4?!", "yeah, there's something very wrong there. that memory consumption isn't comping from the net itself, nd4j or javacpp allocated memory but something is using a lot of off-heap memory. can you open an !issue with your pom.xml, and a way we can reproduce it? happy to help track that down, but at this point it doesn't look like it's coming from our libraries..."], ["i was buiding a voice matching algorithm. just confused about the part you apply a feature extraction. i hoped crf(conditional random fields) would just be my magic wand then came into the conclusion that i should use hidden markov fields. the weirdest thing is that none of them worked. but i saw some hope in svm's. they were some like accurate", "hello daniel", "@ramimuhammed yes?", "yes. please red my question above", "@ramimuhammed sorry, can only do mpl networks for system identification.", "?", "@ramimuhammed i mean that i don't have the knowledge to answer your voice algorithm question.", "thank you. which group would you suggest me", "@ramimuhammed don't know. i'm just focusing on basic use for dl4j. that's my area.", "thankyou"], ["hi, i'm trying to run a dl4j example on powerpc. i experience initialization error when loading nd4j. i created an uber jar and  code  in maven runner. when running the example, i get the following errors :  link . should we consider recompiling dl4j from scratch on out platform ?", "file an !issue about it please, looks like system issue", "code", "libraries found in artifacts are compiled for linux-ppc64le. what compiler is used ?", "gcc cc @saudet @sshepel", "i mean which version", "i dont know, sorry. file an issue. and once proper people come online, we'll sort it out", "ok  issue", "tyvm"], ["hello everyone! i am new to ml and dl4j. hope someone could help me with this: i am trying to add some new data to the training dataset . i used  code  to include the locations of new files and then got error:  code . i guess this is due to iterationorder is not getting updated.", "how can i fix this?"], ["hi guys, i have some questions to start programming and i don't want to follow some internet video that are too general. if all my starting questions will be answered i'll be able to pay 200$ to teach me something more in depth about this world."], ["how can i use wordvectors in seq2seq?  code", "could you give a full stack trace? i know you can't use gist in china so please use tool.lu if possible", "screenshots aren't usually good either", "tough to look at"], ["my app works well using a gpu (sys memory 8g , gtx1080, gpu memory 8g), but using 6 gpus(rtx2070 8g), exceptions were thrown:  issue .", "@alexdblack why is it okay to infer with one gpu, but when using more than one gpu, exceptions will be thrown due to insufficient gpu memory?", "how to release a andarray gpu memeory, let it only reside in host memory?", "i updated exceptions info with dl4j id -beta4 new release  issue ."], ["hi, is there any example to train yolo on spark? thanks.", "how many labled boxes do you have and you need a spark?", "i have close to million images", "no yolo spark examples, unfortunately. it should be essentially the same as training an image classifier, with only a minor difference for the data pipeline. you might start by reading through this:  link", "thanks alex"], ["anybody can help me with this error?  i use that command, but there is no difference! i am using mac os and intellij. the crash is  code  . i ran  code , but there is no difference.", "sorry, but it looks like a common problem. see  link . i can not help with mac os.", "when i use the weka gui with this package i will get a error  code  too .", "as far as i understand, this is a specific problem on mac os. if you use another tool using the same libraries (nd4j), you will still get the problem.", "thanks. is there any specific solution for that?", "i really do not know. but this is definitely the right place to ask. hope you'll get an answer.", "thanks."], ["hi! i just installed deeplearning4j with eclipse, running the examples, works nicely. now i added cuda, still fine. then followed the instructions and copied the libraries from cudnn to the cuda direction, but it is not detected. cuda version is  id , cudnn is id", "output is:  code . anyone an idea?", "did you pick the right version of cudnn ? id is available for various cuda version. are you missing this dependency in your pom.xml  code ?", "thanks for your answer. dependency seems to be ok (as the gpu is utilized): [<-code>] cudnn version ist id", "one with cudnn, one without", "thanks! added it, now works and is 10 times faster @raguenets don't get this chat function that thanks if for you, would have probably never managed without your help"], ["i have a quick question about  link  : why is the prefetchbuffer set to 24 by default when the javadocs on prefetchbuffer says it should generally be the same as the number of workers? 24 seems really high with only 4 workers"], ["hello guys, i have a little problem that need some advice to get it right. is there a way to get the training information into a string or something for each iteration ? since, the scoreiterationlistener automatically display the score using log.info but i need to display the score using a label in my javafx application. any suggestions ? thank you", "quick update, i found a solution just by making my own custom listener class and use it instead of scoreiterationlisterner and it works. hopefully it will save someone some headache. thank you"], ["has anyone used cosine_proximity as a loss function for outputs and if yes, for what use case?", "re: cosine proximity - i've never used it (but yes, it's all tested/gradient checked)... was built partly for our keras import functionalitybut i believe this is one application:  link    link  (compare eq 3 vs the comments of the mathematical form in the code comments)", "yes i thought i could use it to loss compare word embeddings, but somehow i cant manage to set other hyperparameters. i get always nan as score", "gonna check the paper", "and re: masking arrays -@eralyhas explained it well... just to reiterate: the net always outputs something at each time step, even with maskingyou can use this to manually zero them if you want:  link  during training we use the mask to set the errors (loss function gradients) to 0 - the network's predictions are thus ignored for those time steps", "ye@eraly's explanation was good, didn't realized the masking worked that way but its perfectly fine"], ["any idea why this hubert loss function would fail  code ? same network is working correctly for mse loss function.", "it looks like you filed an issue. so others can benefit (especially in google search results) please migrate posts like this over to the forums or gh issues for bugs if you want discussion. thanks!"], ["hello, i've installed dl4j but i need to debug the neural network code. in particular i would like to have access to  code  that i can't find it on the jars (dl4j-examples-1.-beta5-bin.jar and dl4j-examples-1.-beta5.jar). any suggestion?", "source code is in the repository ( link ). deeplearning-nn for multilayernetwork", "thanks!", "you can find the code online  link . it's in the deeplearning4j-nn module, not in the examples  link", "ok thank you, do you know also why during the  code  it does not:  code  i mean, it doesn't print any output for those lines of code.", "are you running this directly in the examples repository, unmodified? if it's your own project, it could be a logging configuration issue", "no, it's an example given in the repository.", "hm... are you getting any other log output? even if there was no data or anything, you'd still get something returned by that  code  call if only telling you that there's no data / results", "and another issue: when i debug model.evaluate() , now, it opens the  code  class propery, but the debugger stops there and i can't proceed. i have the training log. thanks howere for you help. however the official mnist site says: 60,000 examples, and a test set of 10,000 examples. the dl4j correctly trains on 60,000 example but i can't see any test evaluation. pherhaps i miss something", "perhaps", "here's the output i get from mlpmnisttwolayerexample (unmodified, same as the version on github)(i've omitted a bunch of the score listener lines in the log)  link . as for train vs. test:  code  the second argument in that constructor there is \"train\". so mnisttrain is the 60k training examples, mnisttest is the 10k test examples and thus  code ; is the test set evaluation", "my last part of the output:  code  e", "that output doesn't match the example though... i'm assuming you've modified it a bit. anyway, i can't see how you can get no output though  link  you can see for yourself there that even if there's no data for whatever reason, there's still something returned by the stats method", "i've cut and pasted the output", "the example is:  code", "mnist dataset, 60k training instances and 10k test instances", "little indian?"], ["hellow!"], ["hello everyone, sorry for disturbing an open thread. there was one question that i have. java is platform independent. shouldn't changing some parts of deep learning 4j make it platform independent?", "dl4j supports linux, windows, and mac os.", "win32 or ia32 architecture is the one i am mentioning", "win32 is not supported. the underlying c++ is platform-related.", "is there anyway to convert the code? which file exactly causes this? what has c++ to do with java except that java is written in c++", "for efficiency, many functions are implemented in c++/cuda c.", "can they be implemented in java?", "no, the underlying functions are all implemented in c/c++.", "which files exactly?", "which files are not important. it is important that 32-bit os is poor for dl.", "well i am trying to examine the source code and see how it functions to understand dl4j. any suggestion on where to start the source is huge.", "to understand dl4j, the best way is follow the guide directory", "any offline guide for it?", "anyway thank you"], ["hello. i am new to dl4j and this is my first time using a maven project. i am using eclipse with maven support. i have cloned the dl4j-examples project, and did \"maven install\" on the project. i got a success message. however, i am not sure how to run an example such as the \"mlpclassifierlinear.java\" (which contains a main function). right click on the file -> run as -> run configuration gives me this window. i believe i should configure this somehow but this is where i am stuck", "link", "i tried adding the following to the pom.xml  code . i am following the example from this link:  link  and i configured a run configuration as this: when running this configuration, my maven console prompts the following:  code", "i think i solved it. i replaced the original plugin of exec-mave-plugin with the following:  code  and then right click the project. run as-> maven test"], ["hi", "guys how are you"], ["does someone know if there is a way to create a indarray from data in gpu memory (decoded video frame or opencv gpumat or umat) without downloading/uploading to/from host memory ?", "jfyi we migrated most support to the  link  forums so more people can benefit from the responses. it depends on where it's coming from but when you allocate an ndarray in nd4j we actually dual allocate to host memory in the background as well. generally we would do that via javacpp. post your issue on the forum and we can take a more detailed look maybe.", "ok thanks, i'd be interested indeed to know a way to bridge opencv gpumat with indarray using javacpp", "i imagine it would be workable. nd4j directly operates on javacpp pointers. i'm not sure, sam might have to chip in on the answer a bit. i'll ping him when you post. nd4j has an opaque databuffer under neath that handles that normally. we should be able to start with device memory but i'd need to understand some of the details a bit.", "link tell me if something is unclear"], ["hello! i've been having trouble getting dl4j going with cudnn. i'm a maven novice which may be part of the problem; i got the examples going on a cpu backend inside eclipse, but after failing to get it to recognize cudnn by adding maven dependencies (the ones on the cudnn page) i've manually added two jars to my build path, deeplearning4j-cuda (1-1. beta 4) and javacpp ( id -1) - this leads to this follow up error:  code . any pointers on where to go/look next is appreciated, a bit stuck there", "(adding in the cuda windows dlls jar led to a version discrepency error instead, cuda being beta4 and everyting else being beta5, so i'll dig from there)", "they need to be all from the same version, yes, that is 1.-beta5", "the example i'm trying to run is  code , and the error i got first (with no manual jars added, just the attempted dependencies in the pom) was  code . so i did a search for any jar containing that class, which led med to deeplearning4j-cuda-1-1.-beta4.jar - my problem is i'm not finding a beta5 variant of that jar, tried to search online/maven as well. i'm probably barking up th the wrong tree but thats where i got stuck anyhow.", "why do you say it's not available? i see it here:  code", "great! seems to work now. a too quick search-engine search had led me to a mirror site and no the real maven site, and it was out of date. sorry for that and many thanks for the help, appreciated!"], ["(i am new here so if i miss anything don't hesitate to tell me)", "i am trying to make a new listener for my multilayernetwork, but i have a hard time converting the model given as a parameter in the \"onepochend\" event, i was wondering if there was a way to convert this model to a multilayernetwork. i have tried to use the configuration of the model and parse it as json to then create a multilayernetwork with it, but it doesn't work. i've also tried to create a multilayerconfiguration from the neuralnetconfiguration and i think that could be a solution, but i can't figure out how to do it. thanks! i am on windows, use java 12 and maven id", "basically what i want to do is save my model performance in an excel sheet every epoch. i don't know if there would be another way to do this, but a listener was my first solution.", "well i found a way, simply parsed my model into a multilayernetwork lol"], ["hi! i am trying to import a keras model built with functional api. i have tried the method mentioned here:  link . but i cant load,i am getting  code . this is the code  code .", "please post over on the forum and upload your model and i'll take a look  link  ."], ["thank you, raver120,", "i am reading deeplearning4j.org/quickstart,", "your advice is so kind."], ["code  often produces the same sequence in a multithreaded environment. how to solve it?", "code  is based on a thread local design i.e., each thread has its own state, seed, etc. if you want different values in different threads, use  code in each thread", "thank you! i solved it by setting:  code"], ["hello~just 1 question. how can i contribute to document translation?", "you\u2019re welcome", "first of all - thank you for even offering.", "what language?", "i hope to contribute korean translation", "oh my god yes", "so it's just github pages", "link", "here's our korean docs right here", "1 thing that would be immensely helpful would be if you could even just update our quickstart", "before contributing on code, i thought i need to review dl4j project. translating will be good to study.", "oh sure.", "kepricon: is our engineer in korea", "so he can review", "we have 2 people there and little time so any help we can get is amazing", "have you saw our korean channel?   link", "feel free to ask questions there as well on wording and the like", "thanks for help!"], ["hi, none of the documentation links such as  link work, is that a temporary issue?", "go here:  link", "yeah, i found the relevant documentation sites, but all of the links provided elsewhere are broken, the quickstart link is just an example", "mind providing some examples? it might be a syncing issue with our cms gitbook. i can take a look", "for example, go to the main  code  website and click on virtually any documentation link (quickstart, tutorials, support)", "deeplearning4j.org is not our site anymore and hasn't been for a while. please only use  link . if you have anymore issues, please post over on the community forums:  code  so other people can beenfit as well. typically people can't see real time chat (hence why we don't use this forum much or slack) it keeps people from having to re ask common questions", "okay, thanks, will do."], ["i still don't know why i get this: warning: class 1 was never predicted by the model.", "again, can you confirm that you had class 1 in your test set?", "i verified each step and my training data path seems ok", "yes..i have two folders. is face. no face", "track your data input pipeline. there were many changes there.", "code", "how about the model params? anything funky there?", "for nueral net ?", "yes", "everything is the same", "that's strange", "switching to  id  and works perfect", "this is my model", "please don't do that", "use !gist.", "i like how is formatting", "ok", "link", "aha, another change applied is xavier weight init", "originally it was wrong", "now it's fixed", "original implementation is called xavier_legacy", ".weightinit(weightinit.xavier) <----- this thing", "so ...i will changed to ? legacy? the other think that i noticed is standardscaler which is deprecated", "could this change my results ?", "not sure what was changed for standardscaler...", "i mean it's just new implementation available, but don't know what was wrong with original", "i don't know what to follow. to spend time to check if the latest version of dl4j has the native libraries fixed for linux or to keep my  id  version and investigate the part with class"], ["downloaded the most recent deeplearning4j/examples. the following occurs when i run the examples if they are related to mnist. not sure if this is a known issue or class - downloading mnist...<code>", "that's just a network connection problem", "try again, and if necessary, delete the mnist directory in your home directory", ", thank you! i removed the mnist directory and now it is working. do not feel that it is a network connection problem.", "it was most likely caused by a network connection issue while downloading", "thanks, alex. mnistanomalyexample worked and mlpmnisttwolayerexample is working.", "great"], ["indarray myarr = nd4j.create(flat,shape,'c');    what does 'c' represent? and what is the difference between 'c' and 'f'? is there any material about this?", "link  see this", "it's general concept, c-ordering and fortran ordering of elements within linear buffer", "aka column-major and row-major", "yes , i know. 'c' is c order ,and 'f' is f order", "why asked then?", ":)", "see  link", "thanks.", "i am read the examples of nd4j, and i saw this. just ask"], ["when i use this libraries  code  my programm work correctly, but if i use 1.-beta4 i got this error:  code . my net config is:  code . can somebody help me to resolve this problem? thank you", "here is complete error message:  code ."], ["hi again, i switched my ide eclipse from intellij to use deeplearning ui dependencies correctly. now i have an error going like this :  code . my pom.xml is  link  sdk : id", "i found similar issue,  issue , however it is not clear which way should i follow to resolve the problem"], ["hi guys", "i've realized that my /tmp folder gets filled with tons of 'restorexxxx' files, that i guess come from the use of the modelserializer.restoremultilayernetwork(path) function. is there a way to regulate that?", "we can look in to that - could you file an !issue ?", "shouldn\u2019t be. i usually call deleteonexit", "but i\u2019ll look into that now", "ah that\u2019s not me", "that\u2019s probably earlystopping you use there?", "i used early stopping for the trainning, yes", "right, do as adam said, file an issue", "ok", "just file an issue, and we\u2019ll sort it out after release hits. it\u2019s not really a big deal anyway", "ok."], ["\"the webpage talk about input and output masks but only show one layer... is the input mask for only the rnn output layer? or is that for the first/input layer even if you have other non-rnn layers before it?\"output mask is used only with rnnoutputlayerinput mask really only applies if you have one or more dense layers before your rnn layers, as say tanh(0 x weights + bias) != 0 in general... for rnn layers only, it\\'s equivalent to just setting the masked inputs to 0 anyway. so strictly speaking, you don\\'t need input mask for rnn-only networks (but we have no way of knowing that at data loading time)", "only dense layers? not in case of cnn?", "honestly? we probably should have it for cnns as well... but i don't think it's in there currently", "i'm just asking questions i know nothing trying to figure the rnn part out, but probably won't have any meaningful training data before next week", "issue  fyi"], ["hi guys i've just started to use dl4j and i came into an issue with the quickstart page ( link ). according to the prerequisites we should have version id installed for the tutorials. however if i install that version and run the code i get an issue saying not enough arguments for method layer (i can expand on the error if needed). it turns out the reason for this is that listbuilder only has one method for layer which takes an int followed by a layer. once i updated to 1.-beta2 it seems to be working now. i'm thinking the tutorials may need updating so that users install the correct version. (p.s if i use beta5 i get different errors. so it was really just guess work for me to install the right version)", "yes the tutorials are all old versions of dl4j", "thanks. but the version it's asking for in the prerequisites is id yet the code doesn't run with that version and only works with 1.-beta2. has the code in the tutorials been updated without the version number in the prerequisites being updated? also what version should users be using? i would assume id since everything else is beta. but at the same time on the website you link to 1.-beta4. if i click on docs it takes me to a page with latest in the title. but which version is that documentation for? (sorry for all the questions but i'm a bit confused about the versions)", "not sure what version the docs are for, i just know that i had to look at the actual code to see the api, same as you, because the docs were incorrect. if anyone would like to look at an end-to-end example kaggle competition project i built:  link", "yeah it's a shame. it looks like a nice framework (and the only real option for java) but having incorrect docs and tutorials can really make it a pain. i don't really like having to look at the source code to figure out what's going on"], ["i'm trying to apply dropout during testing, as suggested in  link . there seems to be no obvious way to do that in dl4j (or am i wrong?). so, on a computationgraph trained with dropout, i call  code  and get the following error:  code . am i on a completely wrong path? any hints?", "hmm that is an interesting paper. i think the dropout implementation is really not currently meant to work that way. probably want to add a config for .applyduringinference(true) and the layer uses that."], ["hello, i'm using java  id  to run a dl4j deeplearning example with scala, and i've got an error like this  code  but i cannot update my java version because the version on our cluster is  id . is there a method to solve the problem? thank you! here's the line where the error appears:  code", "i'm afraid the lowest java version dl4j supports is 8", "that shouldn't happen, please file an !issue", "i reduced versions of dl4j assemblies in pom from 1..beta5 to id and i succeeded. thank you !"], ["actually while we're on the topic, does remote ui reporting write to file or does the ui put the stats in memory?", "defaults to in memory though, but you can use either", "one sec", "hmm, i wonder if that was the cause of my earlier problem", "i understand it that you can choose, either memory or file", "link", "code  what does attach do?", "to the ui, you don't actually have to display the data when collecting it", "nifty", "i assume a single storage can handle multiple training instances?", "yep, that's the idea", "currently all remote ui info will be posted to a single storage instance", "good to know, that's going to save me a lot of headache"], ["hi, startguide shows code snippet with   code , but in my ide it says that   code  is deprecated, im using org.deeplearning4j:deeplearning4j-core: id  from maven", "what does filesplit do exactly ? documentation is of no help", "have a look at this example:  link"], ["is it somehow possible to reverse engineer the filename from an instance in a datasetiterator? filename of video/image etc.", "yes, you have to enable meta data tracking and then you can get it from the datasets"], ["is the parameter \"learningratescorebaseddecayrate\" the same as learningratedecaypolicy(learningratepolicy.score).lrpolicydecayrate ?"], ["but adam,  i have 1 million records (each has 10 time series), so that means i have to create 1 million files with 10 record in each . i don't think it is a good choice ....", "that's what i do too (for other reasons) and it gives me just a few hundred files (batches of 150 examples)", "(note that i need multiple files for a single example, multiple inputs and outputs, but that shouldn't matter otherwise)", "thanks ede, i will try that"], ["hi adam", "how is it going?", "late in san francisco i'm passing out in a few", "i know.its day in bangalore,india", "yup i usually live in japan", "i'm used to time zones", "1 of those things", "anyways - have a good \"day\" feel free to ask questions, my colleagues are awake", "cool.will do."], ["i am trying to upgrade to beta5 - but i am getting the following maven error:  code  any idea?", "unfortunately we dropped cuda support on mac in 1.-beta4.  link  given apple hasn't supported nvidia cards in their machines for many years, we decided to drop support to minimize development and release overheads. we're not the only library to do this, tensorflow doesn't release an osx cuda version either, for example", "thanks vm"], ["im stuck after reading in the data..like how to pass the data to a cnn..any input into that.", "well, do you have dataset objects yet? that's why i was saying use csvrecordreader + recordreaderdatasetiterator, then it's pretty straightforward", "ok. i will try that..", "can i get back if i have any problem ?", "sure.", "start with this:  link  then adapt it based on my earlier comments (delimiter, regression, etc)", "ok..thank you.. i was following that example to read in the data. i will explore further and will try. thank you"], ["i\\'m trying to use the video classification example and have created a directory with mp4 videos (videos created with javacv <code>. is there an mp4 codec that is recommended, or will any of the mp4 codec options suffice?"], ["when using deeplearning4j-examples, maven can not find artifact  code  in central repository. which repository can i find this? there are only release versions of datavec-spark_ id  in central repository.", "unfortunately it's a known issue -  issue . i have updated the examples, and added a workaround to the release notes -  link ."], ["hi, i'm currently writing unit test for my code. i just noticed that , on my computer, results given by a keras imported model are different in run or debug mode. with intellij, beta- id , cuda  id  , cudnn id . i'm getting different probabilities ([031738654,  id ] vs [51884416,  id ]). any hint to understand the problem ?"], ["hi,", "good evening.", "guys, just go straight to your questions. no need to wait till someone replies to your abstract \"hi!\"", "you'll get answers faster this way", "how to get an image (with rank 4) in a rbm? (channels = 1) my first idea is to flat dimensions 3 and 4 to form img_heigth*img_width columns, bad idea?", "rbm kinda expects vector as input", "so just flatten your images. there's method available for that, nd4j.toflattened()", "ok, thanks"], ["i made an app with dl4j and i want to deploy it on weblogic which is installed on a linux machine", "should be straightforward what are you running in to?", "it complains about class : no jnind4j in class", "just use nd4j-native-platform then? make sure your dependencies are up to date too", "i use a previous version of dl4j  id", "o_o", "yup we've had 5-6 releases since then?", "maybe more", "i would like to test it that version because everything was fine on my machine", "i know..sorry", "we aren't liable for versions before this"], ["hello, i'm using deeplearning4j in a maven project by adding nd4j dependency to my pom file. when i add the dependency, so many jar files are being included. but i only need the following  code . here are my dependencies:  code", "the problem is that when i package my code as jar file, the file size is around 300mb. i want to reduce it. is there any way to exclude all the files which i don't need at once? yes @cowwoc . all the openblas jar files for different os are being included.", "do they actually serve a purpose when a user downloads the jar file in a different os system other than mine? i'm using ubuntu to develop the jar file. will people downloading the jar file on windows or mac require those openblas files?", "i can be wrong but have you tried to play with classifiers for specific platform? also, this issue should help:  issue", "first thing: use nd4j-native instead of nd4j-native-platform the \"-platform\" suffix means \"include binaries for all platforms\". obviously, that'll save you a ton of space. the downside of that is if (for example) you build on windows and deploy on linux, you won't have linux binaries. doing that sort of cross-os deployment with thelatform dependencies will work fine.", "link"], ["hello guys, please a simple question, i have a trained network which is ready and loaded to use it for prediction tryed output, feedforward , predict and i get an exception that tells expecting rank(2) array and recieved rank 1 array !", "figured out how to do it"], ["hello, can you help me for nasnet transfer learning example? i configured resnet50, but i cant do for nasnet? for resnet50 this code is running:  code . i want to code the same that for nasnet", "because the name of the layers are not the same. print out the model.summary() to check the names"], ["is there an easy way to feed submats into a ffn and act on the parent image based on the prediction without having to write to disk?all the examples just show iterator examples. do i have to manually write out the submats to create a dataset/iterator?", "are you talkign about with using javacv?", "yes", "you may want to look at nativeimageloader which wraps javacv", "have you seen that already?", "no, i'll take a look. thanks", "link"], ["investigating why a keras (theano backend) model and the (supposed to be) equivalent dl4j model imported using your new import functionality does not agree on class probabilities for a single validation image. when importing the network config and weights in dl4j i get the following warnings  link . could these be relevant for the differing model evaluations (i am ignoring training for now).", "dropout is the likely culprit. add output(input,false) to your network. the false is test mode", "dont quite follow. change the keras network or dl4j network after import?", "i'm saying when you call output on your neural net. add false to the parameter as a parameter", "ahh, ok", "adding false to model.output gives me the same result."], ["i'm trying to use convolution1dlayer, feeding in indarrays of shape [minibatch, numinputs], and getting this error:  code . changing shape doesn't help, as then i am informed that the shape has to be of length 2. any ideas what i'm doing wrong?", "i've made tests, trying to force input shape to be  code . to no avail, as convolution1dlayer indeed requires a 2d-input. seems to be a bug, as i did a workaround by replacing my convolution1dlayer with simple convolutionlayer with width = 1."], [": i am calling fitpathsmultidataset on spark for 10 epochs. after some epochs one of the container gets killed. what i mean is, after every fit, the memory usage at one of the nodes get additionally occupied with 5gb, which eventually kills the container. i am attaching the memory usage of a single node  link . the sharp dip is when the container gets killed by the yarn , and others are incremental memory occupied after an epoch", "you'll need to tell spark to give dl4j more memory, or make dl4j use less memory...", ": how to do that? i couldnt find any resource pointing to that. in my training configuration, i did enable  code  even when my multidataset is small, after every epoch, the memory doesnt get clear, ideally after an epoch, shouldnt it free up the memory? ie if i call the fitpath smultidataset on a single path, the memory usage keeps stacking up only with every call to fit.", "yes load is intiutive...but i didnt know how to apply it to qlearningdiscretedense contructor, but finally found out...now i have code:  code . and probably the last thing is what class should be put into 2nd parameter of datamanager.load", "if you call  code  after every  code  and memory reduces, then it's probably just the gc not getting called on time. your memory values seem too tight..."], ["i'm using 1.-beta5 rl4j and find that there may be a bug in abstract class policy( code ). the play method has a while loop and in this loop the \"obs\" variable might not be update, always the initial value. please check it.", "it's probably been fixed, please try again with 1.-snapshot, but if not, please file an !issue.", "got it.shall i compile it myself if i want to use 1.-snapshot", "we can use precompiled binaries for !snapshots as well.", "please check out this guide to set up snapshots in your project:  link  .", "okay\uff0clet me check then, thanks."], ["uhm, guys, this should work right? nd4j.ones(2, 2).eq(1); to get a boolean matrix by element wise comparison to a scalar", "yup.eq is 0/1", "okay, because it throws exceptions. i'll create an issue", "thanks", "it works on neither cuda nor native, both with different exception messages", "i've debugged it, and it really seems like bugs"], ["can i get some helps about the comments that i leave yesterday?", "what jvm are you running?", "i am running  id", "you\u2019ll need a newer run to that specific javafx implementation. at least 10.", "oh.. thank you"], ["hi! i got some trouble with dl4j. it's about compiling with javafx. when i run  code  i got the following errors:  link   code", "are you able to run the !examples ?", "yes. i can run the examples with maven. but when i try to run a javafx application for gui use, then it fail.", "how about the image drawer example? that uses javafx.", "i do like this. i will upload it on my github and then you can try to run it and see what's happening?", "well, good luck. i will be off line in a bit. please an !issue if you still run into problems."], ["still struggling to grasp the concepts to design a rnn networklet\\'s say i wanted to detect motions/swipes on \"touch pad\" that consists of n x n sensors that record the state at a given interval. the users obviously do not make the motions identically, including the speeds might be different. so, now i have two questions:1) should i use a single n x n conv layer as input? or should i use some mega large input that contains all the n x n frames until the slowest motion is completed?2) if i used just single n x n as input, how should the training labeling be done? logically it seems that the label should be when the motion is completed, that is in the end, but should the other frames from the first to the last frame also be labeled the same?", "you could use a cnn to obtain a single vector representation and then feed that into an rnn. another approach i think of is to capture touchpad \"activation\" in a single matrix and let them decay with each timestep, so that most recent values will be 1 and old values will be close to zero and then apply a cnn to that single matrix. the problem with tihs approach is that you would need to detect start and end (maybe with another net?)", "could you elaborate on this? do you mean by this that would build input layer that is m frames deep? [m x n x n] if my frame is [n x n ]? because this seems a bad idea, as it would require a lot of computing each step. some kind of hidden markov model would be preferred, when i would only need to know the previous state, not m previous states", "[n x n] -- cnn --> [m x 1] ---> rnn", "so that\\'s my \"1)\" with single [n x n] input?", "yes but with an rnn afterwards", "or did you mean that?", "of course, the beginning of my question implied that i'm doing rnn", "but if i use the [ n x n ] input, i\\'m then struggling with the \"2)\": how do i label my data?when the finger touches the middle of the touch pad, it could then move anywhere, up down left right; so it would seem strange to label it as \"up\" even if it is part of \"up\" motion... or should i? or should those intermediate steps be all classified as \"meh?\" or some intermediate classifier like \"doing up\" and only the end would be \"up\"?", "in other words, in the sequence of \"up\", should i classify the first frame also as \"up\" for training?"], ["hi all, i am moving my first steps in dl4j, and i have a question already: i am following one example of the book \"deep learning - a practictioner's approach\". the code uses dl4j id , and some methods have been since deprecated. two that i'm not sure about are   code  and   code . they are still in 1.-beta2 (and i can run the example successfully) but i'd like to use the latest 1.-beta4. the migration instructions recommend to 'use the pretrain/pretrainlayer methods instead'. my problem is that i can't quite figure out how to migrate as the arguments are totally different (and invoked on the model instead than on the configuration builder). the example i'm talking about is this one here. thanks for any help!", "you can try running the exmple you are studying from the dl4j-examples repository:  code", "- oh, i didn't notice that the same mnist example was also tackled in that directory too. i see there are some differences in the configuration, and in fact the pretrain(layer) methods are not used. thanks for the pointer!"], ["hi, anyone has tried to compile dl4j with graalvm/native-image? i'm using it with quarkus, but no luck at the moment", "it works fine in java mode, but compiling it to native i'm stuck with errors like this one:  code .but i'm puzzled from the error, because i'm running on linx-x86_64 (fedora). and other similar errors for solaris, android and so on...", "any hint would be more than welcome", "hi, i would recommend posting over on the official forums:  issue  was a dependency for this and that works with graalvm now. could you post your use case and we can look at it closer?"], ["hi all! i'm trying to use word2vec implementation using dl4j. i have a maven project and i added the dependencies of deeplearning4j-core, nd4j-native-platform and datavec-api to my pom file. for some reason my class cannot identify wordvectorserializer class. any suggestions? note that i'm using java 7", "solved! i add the nlp dependency"], ["i am trting to load a saved keras model, and the backend is tensorflow. my environment is eclipse, java  id , cpu. this is my code: link  and this is my pom file  link  but it failed.  the error information is : code . i also try to use the git source. there is the same error in the deeplearning4j-modelimport project: code . how to solve this question?"], ["@/allwe're planning on building keras bindings for dl4j. is anyone in this thread interested in and available for contributing (part-time, remote) to that? if so, pls dm me..."], ["hi, is there an option to revert transformations made in transformprocess ? i.e. i have method \"normalize('column', normalize.minmax, analysis)\" and i'm looking for method to revert values to original form based on previous analisys", "and second question i have problem with understanding 'rnntimestep' method. i.e. i have n=100 values in .egression dataset and i wanna predict n+1, n+2 .. n+5. first step is feeding network with all data. next step is pick last data and predict n+1. n+1 will be next value to put inside 'rnntimestep'. after repeating 5 times my data is not even close to real prediction. i dont understand how it works when we put i.e. tensor of all test data to predict real values, how it works in real live examples when we dont have test data and only past values like in first example", "there is a revert method to denormaize a data array. see:  link your second question is at best generic deep learning, unless you are trying to predict simple trends or cycles, it will not work.", "thank you for answer! i know this methods but at the etl stage i dont have this type of normalizers. what i understand transformprocess define internal transform actions called \"dataaction\" based on data analisys. after etl i have normalized data but i lost information about original 'shape of data'. about second question, is there any method/example in dl4j to predict more than one step ahead in regression (based on past data) ?", "can you please provide an example in a !gist , best in the form of a unit test? no, sorry.", "thank you for answer. gist describes what i'm doing. this code work with neural network for stock prediction. predicted values are normalized in range [0,1]. to plot this data i need to revert minmax normalization. i made this using normalizerminmaxscaler but this is not proper way in my case  link"], ["hi! i've been struggling with a noclassdeffounderror on iupdater for a while now. and i'd thought i'd ask for help.", "all the issues/previous gitter post i found were about version mismatch. my pom.xml correctly list 1.-beta4 for both deeplearning4j-core and nd4j-native-platform. i am using openjdk 11 on ubuntu  id  with maven  id", "the cause of the error should show up in the stack trace. could you post the full stack trace?"], ["is it possible to build a (standard) fully recurrent neural network with backpropagation through time in dl4j?", "yes", "because the documentation justs says something about lstm", "oh, you mean non-lstm", "hmm, not sure about that actually", "we only do lstms", "and their bidirectional cousin", "ok thanks", "'vanilla' rnns wouldn't be hard to implement, but we haven't needed them yet so haven't built them", "it shouldnt be too hard to implement that. it just needs the unfolding of rnn into an mlp and then do slightly modified bp", "but maybe deep rnn aren't suitable for practical use, just researching deep rnn for time series prediction", "lstm is very useful for time series prediction too"], ["hi!", "i wanted to know what exactly the score of scoreiterationlistener specify? is it the value cost function ? because what i understand is the lower the score the better. so is it the cost function value or something else ?", "yes cost", "usually for a given batch", "okay, thank you!"], ["hi everyone can you help me? i have a problem with vgg16 transfer learning my exception is:  code . how do i solve this problem?", "hi everyone. i received this error:  code . what is wrong with me? my code:  code", "try adding  code  to your code", "that means you have the wrong input size, that's being carried through to later in the net... for example, feeding height/width 224 to a net that expects size 227 or something in", "thank you, problem was solved. (note: i changed .nin(4096) to .nin(1000))"], ["hi, i'm using dl4j with the model zoo darknet19 where i remove the globalpoolinglayer, it works fine with images up to 3000x3000 pixels. but when i run images bigger than 3200x3200 it gives me unexpected outputs. is there a limit of floats i can input dl4j? seems like is clipping out floats, is something related to the image size", "ubuntu  id , java  id , maven id , scala id", "please try with 1.-snapshot and if it still fails, please open an !issue with code and data", "your pics are resized to fit darknet19 input layer, and on big pictures it seems that bounding box predictions simply give you nas. i don't quite think that image size has anything to do with your network topology.", "thanks for your answers. i solved the problem using dl4j version 1.-beta5"], ["running into warn  code  when trying to deploy. relevant  code  and contents of  code :  link  . this is running on ubuntu  id  on a p id large instance on aws.", "i've tried both a \"plain jar deploy\" (a zip file with my play application, coming out of sbt dist), and in play dev mode, \"sbt run\". added the relevant /lib directory contents to the gist.", "haven\u2019t installed cuda or cudnn as i was under the impression that the redist jar would have all the necessary stuff in it", "@alexdblack logged:  issue"], ["hi anyone can help me with convolution.convn getting unsupported exception", "can you post your message as gist ?", "hi  i am trying to use convulition from nd4j but getting unsupported exception. here is my code  code", "you are doing a lot of things in one line : convolution, logging, printing. what is the full error message ?", "code  i am trying to achieve implement a demo, if it works then i will play with it", "don't you have several lines of errors ?  reading  code  it looks like dimensions may not be correct.", "check updated gist. not sure , but from python numpy , it is correct , but when i try to implement with java it is not working", "i understand but dl4j is a java framework that uses java conventions.", "it is dl4j or nd4j?? i saw 2 diff lib are there", "dl4j is a set of libraries. nd4j is one of them. a \"numpy-like\" library.", "so which should i use if i want to use all the numpy function in java, and where i will get the example, can you please share some reference", "try to read this as starting point. code", "didn't find anything related to np.convolve", "have you seen my private chat invitation ?"], ["if i have a dataset iterator that contains the data for a csv file, how would i split that into test and training datasetiterators using dl4j?", "that\u2019s a question best asked on the forums. it\u2019ll help people find it via google.", "would splitting a file into test and train sets before i convert them to dataset iterator be a viable option?", "it\u2019s an option but not the only one  link .want to give this example more google juice.", "youre a legend, thanks."], ["hi! anyone still here? how do i implement huberloss function?", "by implementing iloss.", "thank you. is there any example?", "yeah. there\u2019s  link  but with samediff now you should just be able to implement a loss function much easier using  link . don\u2019t yet have an example using samediffloss though. pull requests appreciated there.", "thanks. btw  are you guys going to implement the huber loss function in the near future?", "it already exists in samediff and nd4j\u2026 try this?", "i will. thanks eduardo"], ["i did investigate that first but it seems that it doesn't really perform a sliding window, and then i'd have to convert each window to an indarray anyways which scares me", "well, it's kind of limited in what it can do, but it's fast"], ["hey there, i try to build a own computation graph for the bertiterator. my try wath with the embiddingsequencelayer and an lstm layer as output i used a rnnoutputlayer. but i got the error: expected rank 3 labels array, got label array with shape... is this in cause of  issue ? or can i use another kind of outputlayer to get my model working", "ok found one step:  code  but now it has trouble on the back propagation on the embedding sequence layer.", "mind posting over on the forums  link ?", "no problem made an entry"], [", can you look at the genderdetection example code that i uploaded few days back for your review? please let me know if i need to add more useful information into it to make it compatible with dl4j-examples.", "i merged it already?", "did you do another commit?", "no, i didn't.", "is it available in dl4j-examples now?", "link", "oh..", "great. i am happy to see it"], ["is there a way to calculate how much time it would take for word2vec to fit on some data? or even monitor what's going on? the reason why i am asking is because it has been more than 1 hour 20 minutes since the last log entry of  \"starting vocabulary building...\" and 1 hour since it finished iterating over the data. ps: there were 58754 sentences for training the model", "with a small data set like that, it shouldn't take long you can add a vectorslistener instance to track progress... it should also log plenty of infobtw,", "what version of dl4j are you using? and what backend?", "dl4j id with nd4j-native. there's factorie and jsat, both of them are regularly updated", "i'd suggest asking @raver119 about the word2vec performance issues (he should be online in about 6 hours maybe?)fwiw it should be significantly faster than id , and your data isn't largefeel free to open an issue though with details and configuration - that might make things easier for us", "sure. btw, each sentence is a pretty big (almost a complete email/document) so that could possibly be slowing things down. and it's running on an 8core cpu."], ["hey everyone! i'm a student working on a final project for my undergrad machine learning course, and i'm trying to use dl4j for cnn image classification on this dataset:  link . i've gotten the network to run well on the cifar-10 dataset, but i'm having a little trouble switching over to this new one since there's no built-in datasetiterator. how do i have to format the image file directory to work with the imagerecordreader?", "look like your bird dataset contains images within folder, the folder name being the label. you can use similar appraoch [<-url->.]", "thanks! that worked"], ["hi. i'm trying to use tfgraphmapper to import from tf. i'm getting: \"no tensorflow op found for batchmatmul id  possibly missing operation class?\"", "can that just be added to mmul's tensorflownames, or is there an incompatibility there?", "i don't see any differences in the tensorflow public apis for batchmatmul vs. batchmatmul id", "yeah, tf has a habit of renaming things for no reason. we've already run into and fixed that on master but yeah, for beta4 what you've done is probably exactly what i'd recommend as a workaround"], ["hi, guys. sorry about this naive question, but can anyone tell me which dependency has the uiserver class? i though it was the deeplearning4j-ui, but it isn't there...", "that should be  code . how do you find it is not there?", "thank you . however, it does not hold the uiserver class... can i post a picture here?", "link", "vertx? trying... perfect, . after adding the vertx dependency, the uiserver is again available. shouldn't it be automatically added through deeplearning4j-ui?", "no idea i haven't set it up in some projects others than examples but in the examples it is not explicitly requested", "nice!"], ["anyone know what would be a common cause of multilayernetwork.output() returning nan?", "thanks guys"], ["if the loader is nativeimageloader(224, 224, 3) why would  code  return a shape: [1,3,224,224]. setlisteners(#deeplearning4j)", "is there an irc channel for dl4j?", "chat is a little annoying for the issues you typically get with deep learning. the discourse is where it\u2019s at. re: 1, 3, 224, 224. nativeimageloader returns things channels first", "ah i see, alright. ok, thanks", "there\u2019s some issues in the last release with regards to the channels though. that are fixed on master. usually you could work around them by permitting things though.", "ah good to know"], ["how to get layer instance of a specific vertex in computationgraph from custom baseinputpreprocessor... ? i want to simply reach the weights of a layer (vertex) from another layer (vertex) during training..."], ["hey is anyone online?", "just ask your question please."], ["hi everyone (hopefully) quick question. my understanding (please keep me honest!) is that in dl4j (and most other ml/dl frameworks), all of a particular neural network's configurations (layers, weights, misc parameters) are just data configurations, and as such, can be saved to disk in a file. that way, i can tune a network to work exactly how i want it, store its configs/settings in a file, and then reload that file at a later time to reproduce the exact same network later on. if this is true (and correct me if its not!) then it occurred to me that it would be super helpful if these \"network config files\" had a standard format. that way, i could train a network to learn how to spot a bowling ball in an image using one directory  export the file, and use it down the road in another project that perhaps uses a totally different tech stack. but because both frameworks support this open standard, both would be capable of loading and configuring the exact same neural network that will correctly identify \"bowling balls\" in an image. does such a standard file format exist, and does dl4j support it? thanks! onnx perhaps?", "onnx is the closest thing out there to a universal format. but it doesn\u2019t support everything.", "thanks  , googling dl4j + onnx i see some evidence of support for it in dl4j but its mostly just javadocs that pop up. do you know if dl4j allows for onnx-based exports, and if so, how comprehensive/developed that feature is?", "not sure if we have export yet. but import is coming.", "thanks, and finally: is it fair to say that dl4j stores more network directory  in its own serialization mechanism than what onnx currently supports?", "hard to say... onnx does support quite a lot of ops right now. i\u2019m pretty sure samediff has more ops than onnx supports, but for the high level layers in dl4j... i would guess it has them all.", "ahh, samediff is what dl4j uses then?"], ["hi, i am running the example [96] org.deeplearning4j.examples.convolution.objectdetection.housenumberdetection. it's in the training phase and i was wondering why this sentence keeps coming up: class - score at iteration 352 is  id . specifically, i do not understand the number 352, in this case, if by invoking the sentence that causes the above, that is, this  code  nepochs is instantiated as  code . from ignorance and wanting to understand, shouldn't there be at most 20 scores?", "may be you are confused about iteration and epoch. see  link for example."], ["hi, i'm running into a problem when i use cuda for my model with custom dataset. when i use cpu to run my model, it works fine, but when i use nd4j-cuda- id  with my 2060super, my evaluation metrics are just all 0's  code . when i run example code with cuda it works fine. when i use my same model but with a dataset like mnistdatasetiterator it also works fine. so it must be something with my dataset. are there some restrictions or gotchas with cuda only? again, my model works correctly with cpu. thanks!", "figured it out with the help of adam. it was an issue of using dataset.shuffle() with gpu."], ["i'm training a dnn + cnn model. the f1 score is around 50. but there are some warning that some classes are never predicted. i did a quite check. i guess that there are around 50% of training data is labeled as type a. there are 5 types. shall i create the dataset which is perfect balanced for 5 types?", "what do you mean \"dnn + cnn\" model? if you trained a conv net say that in plain english", "beyond that yes, minibatches should always be balanced", "the intuition for minibatch learning comes from statistics", "if you have a minibatch it should ideally be as close to representative of your whole population as possible", "i mean danse layer + cnn. i used wrong word, i guess....", "the reason i ask about your model, is because batch size largely depends on dimensions of the data and kind of problem as well as number of labels", "then say \"conv net\"", "so depending on how big your images are you could be like mnist where 1k is a good idea (small dimensions) or 32/64 like imagenet", "the label distribution will come down to batch size", "ok. i merged a csv data and image data 100x100 pixel", "but as evenly distributed as you can get would be better, make sure to check our !tuning guide", "i read it, but want to learn more, for example about batch balancing", "thanks for the answer"], ["@agibsonccc the class baseoutputlayer, 425 line, bug:  code", "given our very thorough unit tests - i doubt that's a bug. got a specific example that it actually fails on?", "it should be obvious here : return labels.reshape(labels.size(0), labels.size(1)); //line 425", "that would be a no-op i think", "when i use \".inputpreprocessor(3, new rnntofeedforwardpreprocessor())\", the label was converted into 1d, my timestep=1. but it can work in regression.", "post your full configuration in a !gist", "gist cant use in china, can report a bug? it looks like a bug", "i'm not yet convinced - but sure, open an issue, and i'll take a look", "ok", "issue", "thanks"], ["i have still problem with understanding the batchsize. let me explain, if i have 100 rows in train set and if i set batch size=100  means what? or setting batchsize=1 whta are the differences? i really still dont understand.", "k", "so i'm going to ask you read a book now   link", "ok", "link  go through this chapter and come back", "if you still have questions then we can talk", "i'm biased but i also wrote a !book", "thanks you very much", "more basic questions are answered in depth in the textbook", "ok"], ["hey guys. i am facing the following exception when in run dl4j from eclipse  code . any leads ?? please ??", "there might be problem in version you are using it might be not compatible.", "thanks bro !! any thoughts on how to chcek it ?"], ["hello alex, just a remark about u-net zoo implementation, in the original paper ( link ) the last convolutional layer is 1 by 1 convltuion but in the zoo ( link ) implementation is convolution of 3 by 3.", "mind opening an !issue for that? thanks", "i opened an issue  issue ."], ["hi, happy new year ! i'm still trying to train a custom cnn model without success. loss is decreasing. deviations are between [-1,1] , but final results are nearly black pictures. how can i debug things ? for i know this model works fine in python/keras. unfortunately, i can not provide it this time.", "is this for segmentation? i'd need to see your model configuration to say more... but one thing that comes to mind if it's basically all black is conversion from 0-1 range (sigmoid output activation function) to 0-255 for display", "thank you for your answer. it is for segmentation. i correctly multiplied by 255 for display.", "here is the model i'm trying to train to do segmentation. may be i'm doing something wrong but i can not figure it out. this model works fine in python/keras  link .", "here is the learning process in action. it does not look too bad."], ["hello i'm a french computer science student and i'm starting to use dl4j. but one exception (among others i guess) gives me a hard time:  code . i have a csv file with 2 columns and looks like this (with about 30,000 lines):  id ,2  id ,5  id ,6  id ,1  id ,7  id ,3  id ,9. the second column represents the label and the first one a value (any). my model configuration is as follows: does anyone have any answers or advice", "you set your numinputs to 2 but your input is only 1 column of feature.  rmb to onehotencode your label if u r doing classification", "thank you, i did, and i don't do classification. i'm working on time series, but i have other problems now in relation to that. and i'd like your help... so here's my problem: at the beginning i had a csv file with 3 columns as : date, averagepersonsick person, cityname. my goal is to predict the number of sick people in city x compared to the data of all the surrounding cities. and i converted it into : averagesickperson, cityname because the following error appeared:  code . but that doesn't solve my problem ... because the result doesn't seem relevant to me: photo above. how can i (re)start to have a basis for work? thank you"], ["i have a large number of images on hdfs. is there any method to load these images as  code ? i have a trained cnn model that i will use to predict image class using  code", "this should work for hdfs images -> rdd of indarrays:  code", "thanks alex."], ["is there anything in datavec thta would help me create separate feature/label files, like the ones used in the uci classification example, or will i have to create them \"manually\" for my own time series?", "you'd have to do that manually but if you have a vision for how such a feature would work feel free to file an !issue", "something like that wouldn't be out of the scope of datavec"], ["i'm trying to do image segmentation on 1024x1024 images taken from airplanes. i have a rtx 2080 im using for training, which has 8gb of vram. i am unable to train on the full image size unless i process one image at a time, which seems bad. if i downsample the images to 512x512 i can run batch sizes of 4. do you guys have any recommendations? this is my first time trying image segmentation. are there any tricks i can employ to reduce the image file size without hurting the network learning? what are the best practices? initially i was thinking of splitting the image into four quadrants (1024x1024 into 4x512x512), without any downsampling, and train on those. but my results did not seem to improve much.", "lookout for firebase auto cloud image resizer that has been released...", "was that for me?", "yeah  ... i think u can use that without distrubing network...", "does that just resize the images? because i can do that fine on my machine"], ["hi. is there a place for deeplearning pre-trained model (not only model, but weights)? i am looking for a way to use pre-trained vgg16 in dl4j. any pointer?", "we don't have weights yet. the plan is to add that to:  link  when we do"], ["hi i am having a little bit of trouble setting up all of the dependencies for spark. with this import line:  code", "make sure all your dl4j versions are the same first of all", "you saw our !examples right?", "those has the pom.xmls you need", "usually it's just dl4j-spark with the right scala version. dl4j-spark_ id  or dl4j-spark_ id", "yeah all of my spark import statements work except for that line. i am getting cannot resolve \"paramavg\".", "i will look more into that. thanks adam."], ["good morning. i am curious about the input data needed to train cnn in the medical field. need labeling like bounding box, polygon? if necessary, which of the two do you need? do i just need an image? do i need coordinate values \u200b\u200btoo? plz help me", "depends but polygons are used a lot. but the model usually outputs per pixel.", "thank you~~"], ["i have a brain freeze about recurrent nets... all i'm trying to do is to evaluate a test set that is a timeseries data... and i can't figure out how to do it? do i have to feed the data one by one? because if i try to do it the \"normal\" way i get mismatch in label and predicted dimensions", "using the evaluation class?", "you want evaltimeseries for that", "oh", "for the net, just use .output(indarray features)", "let me digest this for a moment", "link  actually, just do that multilayernetwork.evaluate(datasetiterator) if you can", "something is still going wrong, let me clean up my code and post it as gist", "so if you remove the statslistener you'll still get that output", "you're saying the scores actually change if you do that?", "what i'm trying to do is recurrent with one input and with 4 classes:  link", "ah, use an rnnoutputlayer, not an outputlayer", "ah!  thanks a million", "sure, np"], ["my computer does not have a gpu. is it possible for me to build the whole project locally?", "see 2nd comment:  issue", "that means \"building the cuda backend\" is not necessary?", "as long as you skip it you don't have to", "is there a reason you're building from source though?", "you know you don't need to right?", "i want to change the loss function.", "you can add a custom one without compiling nd4j though", "for some special reasons.", "issue . it doesn't matter what they are there's an example right there of how to do it without compiling dl4j", "our model is very special", "you can also do custom layers", "is there some instructions i can follow?", "on what? custom layers?", "yes", "again - please listen to me when i say file an issue with what you're missing", "we have examples", "if you want step by step tutorials tell us what", "we have a dedicated person who does that stuff", "they read issues", "link", "custom layers and custom  loss functions", "we have basics", "the readme.me is right there for it", "readme.md*", "if you want more then tell us", "okay"], ["how can i implement this cnn in deeplearning4j. what layer is concat", "can anybody help me?", "concat is the default of what happens when you give one node multiple inputs in acomputationgraph.", "thank you. how can i implement the three convolutional layers in parallel like the image. deeplearning4j supports this operation. could you give me an example is that i am new and i want to mount this example.", "i think there\u2019s an example for it\u2026 somewhere. don\u2019t know where it went but it was something like this:  link", "thanks"], ["15 learning rate: link .  id  learning rate: link  i was looking into the rnn regression example and i am a little confused by one of the results i got. the model reports on the mse of the test dataset. when i run the model with a learning rate of  id , it has a mse of  id e+00 and the graph is nearly perfect. when i run the model with a learning rate of 15, it has a slight lower mse of  id e-01. i would expect the graph to be about the same or slightly better due to the lower mse, but the graph with the model having a learning rate of 15 is worse. this just doesn't seem right to me, can anyone explain it?", "i just thought a lower mse meant a better fit of the data, and i don't understand why that isn't true.", "so the model reports a loss which is also an mse. the higher and lower numbers you are quoting, are they the loss?", "no they are not the loss of the model. they are the mse when the model is evaluated on the test data. it is what is printed by the last line in this segment of code  code . i believe that these mse numbers should directly correlate to performance on the test dataset.", "ah, i bet the mae reported is opposite trend, is it? those stats are collected on the scaled output between zero and one.", "both maes are extremely close(2 apart). but they are same trend.", "yes they are collected on the scaled output,", "could that be causing the issue?", "no. that's not an issue.", "okay, any idea why the mse is lower with a worse result?", "it's just that it throws of the scale a bit. i have to look at the code. on my phone atm. i'll look and let you know.", "thank you", "this might be better as an !issue? we could frame this as something to verify with the gui", "want me to report it? also what do you mean by verify with the gui?", "the charts might be off for some reason? i mean it could be anything, frame it clearly as ambiguous. i'm just saying in general, it could be a scaling issue, the data. neither of us is sure. hence why she's investingating", "ok. makes sense", "it's mostly \"one of my engineers is looking in to a possible bug and we aren't sure what, but this chart looks weird, we're not sure why\". \"let's log it to acknowledge something is being done and also just remember that this happened\"", "it is a time series prediction example. the result at lr 015 just happens to come closest to the actual result.   .", "typically a lower mse by definition is closer to the \"true\" though", "the question is \"if it has a lower mse, why is it farther from the actual result?\"", "that seems like an obvious thing to have right", "there is a permute in eval time series and", "case might hit the corner case of tad with one dimension equals 1. there is only one feature in the time series.", "could be a known bug.", "that adam is working on", "subtle but possible", "is the bug simply cause evaltimeseries to incorrectly calculate the values? and is there a way for me to test and confirm if it is the bug?", "let's just file it for now and we'll double check this", "sounds good with me. i will file it.", "thanks"], ["hi there! we are using dl4j to deploy keras models (cnn) into an java environment without gpus. the inference time is quite slow and we would like speed up by using multiple threads, we tested the parallelinference wrapper quite a bit and couldn't really get any performance boosts. we tried the different inference modes (sequential, batched, inplace) and fiddeled around with the batchlimit. any recommendations, hints about what might be the problem? we are currently using beta4, could changing to beta5 help? thanks in advance!", "cnn without gpus will work slowly. have you tried avx (if your cpu are recent) ?  link", "hej , thanks for the answer. i'm aware that inference on cpu will be (much) slower than on gpu and that there are several ways to improve cpu performance by installing libs that improve the vector operations. i was just wondering if it is possible to scale up by using multiple threads as well.", "did you profile your code ? can you see where bottlenecks happen ?"], ["hi everyone, can i make a text extract tool for my pdf documents with dl4j like  link . i need detect paragraph, head of the text, and signer of the text with deep learning, any advices or samples? thank you", "yeah use yolo. works surprisingly well. then crop and use tesseract.", "actually i dont need to ocr, i have extract text,i got the text with apache pdfbox/java pdf library, but template of the documents are different, for example signer of the document names in different position, or head of the text in multiple lines ext. so i need a model to which can learn from documents has body, head, signer name position etc. and detects them. yolo i think real time/ i have just pdfs", "?", "yeah, my advice is still the same. try using yolo."], ["hi everyone, i have a problem with cuda  id  and 1 (i installed cudnn manually) using snapshots. beta4 is fine. the error is  code . i used snapshots up until 2 weeks ago with no problem. i'm using a 980ti on win10-64", "could you provide more information about this as an issue on github?", "ok, thanks."], ["hi. i don't know if this is a good place to make this question but here it goes. i am doing  code  and loading the model like this  code . i am getting this error  code . how this is happening if is the method itself that does grab the matrixes?", "sorry, that's known issue and was already fixed... it's caused by new 0d/1d array mechanics introduced in beta4"], ["hi, everyone! i wanna to implement some missing functions in deeplearning4j (for example the multiplication operation in elementwisevertex). is there a recommended way to do? thank you!"], ["about the crash, i'm running the following 4 lines of (kotlin) code in multiple threads: code . i'm seeing the crash with openjdk8. i'm not 100% sure if it crashes with oracle jdk as well. the crashing runs were a spark job on emr and they use openjdk. i can try oracle locally and let you know in a few hours (takes quite a while which is why i wanted to parallelize).", "!issue please.", "that stuff shouldn\u2019t pay attention to jvm being used. but \u00abspark\u00bb word makes it suspicious. how you\u2019re sending things to spark?", "the other big difference in that environment vs. my laptop is # of cores. i have only 2 (each hyperthreaded) but the spark machines have 8. wonder if that just makes it easier to trigger this.", "and if it crashes the jvm it is most probably a libnd4j bug or a problem between nd4j and libnd4j", "hmmmm.. it seems to have just passed the turing test", "will file bug on nd4j then. will try to reproduce on my laptop first. that'll take a few hours.", "file it even before reproducing, and add your results after that. it looks like your crash may be due to a race condition that only shows itself in a highly parallel scenario, so you can wait a long time before you see it on your laptop, if ever", "ok. will do."], ["how to obtain epsilons in computationgraph?", "@llzd010 do you mean gradients at the input/features? or do you mean for a specific layer within the graph?", "yes l/input", "gradients at the input/features", "@longzhendong hm... i was sure we had something for that, surprised we don't, i'll open an issue later. we have multilayernetwork.calculategradients which does exactly that as for a workaround: hm... it's a bit of an ugly hack, but you could add a custom inputpreprocessor then capture the gradients during computation. so add the preprocessor, during the preprocess method (forward pass) pass array unchanged (via  code ) during backprop method, detach and store the output array, for user later", "i did it the other way, but there was an exception  code . the value cannot be copied", "i.e., use  code method", "code  \uff1f", "no, i mean use detach within the inputpreprocessor. doing it your way won't work, the workspace it was in has already been closed by that point", "can you give me an example\uff1f", "2 methods to implement. literally preprocess method should be return  code  and backprop method should be 2 lines:  code", "3q thanks"], ["is there an env variable that i can use to specify which blas to use (mkl vs openblas)?", "if mkl is in your library path, it will get used, but you can disable that by setting the class system property to an empty string", "like this?java.library.path=\"\"", "yes, that's one way", "hm, doesn't quite work.  works like this though -$ ld_library_path=\"\" java main", "there was this bug with  code  have you guys had a chance to fix it in master?", "i'm trying to compare mkl vs openblas performance in my case", "sure, it's only a build issue with openblas  link", "so when i build from source - does the build also discover mkl automatically and link against it unconditionally?", "does this happen when building libnd4j or nd4j?", "lib", "in my use case openblas is 3-4 times faster than mkl", "surprising.", "yeah.."], ["hi, i find that  code  costs a lot of time, is there anything i can improve it?", "there's lots of  code  methods. there is some initialization cost on the very first time you use it (loading native libraries, etc), and it should be faster after that we'll need more info to help more than than... like how long is it taking? what create method signature specifically? cpu or gpu? what array size?", "yes, sorry i find that it's only cost more than 2 seconds at first time, then only 2-10 ms, that's great", "ok, great"], ["raver120: is there any chinese edition of this book?", "raver120 is a bot", "we are translating it to mandarin with oreilly", "but it won't be out for a while", "if you want more information add me on wechat", "", "ok", "i can point you in the right direction", "we have a wechat presence", "we also do a lot of business in china", "thanks a lot", "i begin to learn dl4j for a little days, and  have lots of questions to ask.", "thanks for your patiences", "wechat?the chinese im tool from tecent?", "yes i use wechat actively", "i was just in china for all of november", "i live in asia", "i spoke here just recently:  link"], ["hi, one quick question  you run uiexample from intellij idea as java application or maven something?", "what do you mean?", "i open the uiexample.java file all good", "i go run as java application or i need to do do something like maven package?", "run as java application", "ok, tnx!"], ["hi guys, with deeplearning4j can i reproduce the results achieved articles about artistic style transfer? i found several implementation on tensorflow (e.g.  link  ). is there any similar example or tutorial also for deeplearning4j?"], ["good morning! i continue my question re. lstm keras import. output from summry:  code . it's ok when i feed 3 dimensional array and call output, but when i feed [1,6] array, i am getting  code . in my experiment i an trying to get rnntimestep to work with imported model. thanks!", "so to be clear, you are calling rnntimestep, and [1,6,1] works but [1,6] doesn't? possibly an edge case issue with lasttimesteplayer, mind opening an !issue? in the mean time, just use rank 3 input instead", "thanks alex!"], ["hi, i'm trying to import a keras model (trained with keras id -tf) with  code  . but it is giving me a nullpointerexception in function mapoptimizer in file kerasoptimizerutils.java. this is due to the fact that the variable optimizerparameter is holding the learning rate parameter with key \"learning_rate\" but the code is looking it up with key \"lr\". has this been fixed in a snapshot maybe? thanks!", "doesn't look like it, unfortunately -  link . presumably that is a recent keras format change. mind opening an !issue and we'll take a look", "sure thing, was just about to do that! i'll post a quick dirty fix as well", "all yours   issue", "thanks, we'll comment on the issue once we'd looked at it"], ["hello, perhaps someone has seen this and would quickly now how to fix this. i imported a mobilenet model i trained in jupyter on some images, and now i successfully imported it in dl4j but when i try to do a prediction on a image i try to load it seems the shape is different than what the model expects as input. so if i provide as input = new nativeimageloader(224, 224, 3) that translates into a shape: [1,3,224,224] which does not work. but if i create an input from the model import dl4j docs like input = nd4j.ones(1, 299, 299, 3) this translates into a shape: [1,299,299,3] which is accepted by the model. so my question is how could i load an image in dl4j so that it is loaded as [1,224,224,3] instead of [1,3,224,224].", "ok, i think i found the answer,  code  thanks"], ["hello, i am confused what version now i should use for cuda - cudnn. here  link  you said that: your dependency declaration will look like:  code . as of now, the artifactid for the cuda versions can be one of nd4j-cuda- id , nd4j-cuda- id  or nd4j-cuda-1. so we should use nd4j-cuda-1 or nd4j-cuda- id ? if we use  id  what version of cudnn you support now?", "what is the output of nvcc --version ? that will give you the cuda version number you should use.", "id  is version. what version of cudnn i should use?", "cuda- id", "ok version of cuda i should use is  id . but what version of cudnn  id ,  id ,  id ...?", "i don't know. sorry. id according to the nvidea downloads page. (but have not tried)."], ["hi, i'm trying to solve my training issue with zoo unet. having nan score , i decided to change my batch datatype to double. i create batch with  code . i looks like using this leads to a cuda problem. i do not understand why..  link  setting to datatype.float is ok but leads to nan score.", "i would appreciate having examples of training to improve my knowledge of dl4j. do you have a working example of training unet with images from isbi challenge ? searching the dl4j gitter, it seems that other people already got in trouble with this."], ["hi everyone! quick question, should i be able to run the rl4j-examples inside the deeplearning4j -examples by just following the quick start guide? i'm asking because i keep getting error when try to run it", "can i know where you got the examples from? am trying to a reinforcement code in java as well", "it's available inside the deeplearning4j-examples github when you clone it.", "thanks man", "please do inform me if you're able to run it as i'm facing some trouble with it.", "alright man"], ["hi, i want to use dl4j to train my model, but my big concern is about training speed. i just read an article  link  , seems dl4j is kinda slow. i am wondering whether this article is misleading somehow?", "that stuff is always out of date", "i would run your own benchmark", "all that stuff changes each and every release", "when was the paper even written? o_0", "link", "october 2016? not sure", "hmm..may", "run your own benchmark if you do make sure to pre save the data:  link", "ok, i will try to find out"], ["i thought someone at dl4j would like to know:  the predictgendertrain  dl4j example frequently does not learn/converge (~50% error through all epochs).   it seems to work reliably when i reduce the   learningrate = 05;//gf was .01      so, you might update source code.", "pull request would be even better", "fyi: when it doesn\\'t converge, it repeatedly gets:  \"o.d.optimize.solvers.baseoptimizer - hit termination condition on iteration 0\" errors.", "sorry, i'm not that sophisticated with git yet.", "link   doesn't hurt to learn it's only a few commands  link  it's widely documented", "ok, wow, i did it as a pull request.  that wasn't as hard as i thought.   you can quote me on that for future people.", "haha awesome. congrats!. merged welcome to the world of open source", "this is awesome.", "thanks for giving me a push, adam. i mean a pull. or whatever.", "heh"], ["question about dl4j-examples with cuda. i try to run example using gpu but always get this error:  code .", "nd4j-cuda- id -platform, version 1.-m id  , gpu: quadro m2200 tried  id   id   id , always same error. thanks !", "mind migrating your questions to the forums?   link  it helps prevent repeat questions."], ["word2vec: i have 2 words as vector representations. after i averaged them, i have one vector representation. how do i lookup the table with the vector representation?", "the same way wordsnearest", "it accepts vectors as well as words", "where can i find a decent documentation containing all the functions relevant to word2vec?thanks!", "in javadoc  link"], ["hi - are there any working exampless of how to use workspace? the doc is unclear.  i thought beta5 was not available on mac os ? how did u get it to run?", "i just had to downgrade gcc, as described here :  issue . and fyi, i observe the same problem with beta4", "and i also tried switching between openblas and mkl, but it doesn't change a thing"], ["hello, where can i find an example for stocks, preferably using rnn, lstm, gru, thanks!", "we do not have a stock price prediction example in the dl4j-examples.", "thanks ! any thoughts or links to start out?", "we are leaving the realm of dl4j (where this channel should be limited to.) but have a look at this:  link"], ["build failure with 1.-snapshot caused by:  code", "you'll need the libraries for cuda 1", "my pom:  code", "how to use directory  and what about notbits?", "build failure with 1.-snapshot caused by:  code", "re: cuda unsatisfiedlinkerror - as samuel already said, make sure you've got cuda 1 installed an on the system path. if you're sure it's set up correctly, post the output of this:  link  as for bitwise ops: something like this:  code  actually, also a few of them have java ops too - you can use those with nd4j.exec  code", "thank you!! it work fine with beta4, so cuda is setup correctly. i don't find bitsand and other of them bits ops in the linked you gave. what are their class names?", "i don't think we have a java class for that one yet, but you can use the dynamiccustomop.builder approach i showed above", "what are their ops names? \"bits_add\"?", "you taught me this usage. but i don't know the names of the operations, and i can't use them.", "what are the names of bit ops(bitsand, bitsor, bitsxor, bitsnot) used in dynamiccustomop.builder ()?", "or where can i find them?", "these ops aren't merged yet. will be merged later today", "ok, thank you!"], ["hello, i've been using dl4j for a while now, mostly with rnn/lstms. i have just migrated from beta2 to beta5. and i noticed a serious degradation both of performance and learning rate. i reproduced the problem by simply taking the basicrnnexample, and change the dl4j/nd4j version back and forth. i consistently see a huge performance impact (2-3 times slower with beta5) and a slower learning (for the same number of epochs). all tests done on osx : code . is this a known issue?"], ["did i just dream that there were aws amis available for dl4j? can't find any. perhaps it's just not in my region", "we don't have anything up to date up there - we do have the docker containers though:  link"], ["recently i tried word2vec from dl4j, but i found some bugs! firstly,  link  in line 114 \"words.get(a).getpoints().add(words.size() - 2);\" here is not correct. the original code from google(c language) shows \"vocab[a].point[0] = vocab_size - 2;\" the meaning is very different. and i also do the experiment, it is really not correct. also, i tried the spark version of word2vec, it seems problem is more serious,  link  the biggest bug is line 151, \"indexsyn0vecmap.put(currentwordindex, randomsyn0vec);\" here should be \"w2.getindex()\" instead of \"currentwordindex\". also, in line 114 \"indarray randomsyn0vec = getrandomsyn0vec(vectorlength);\" here should not always create random vectors. it should be like this:  code . also some other parts of spark version of word2vec are not very ideal. anyway, i think spark in-built word2vec (scala) is also not good. because it just use sum of local lookup_table to get global lookup_table directly. i think it should use mean or some other ways. so that's why i study these different version of word2vec.", "file an issue.", "meh, not sure what happened there, releasing can be weird sometimes."], ["a further question.  i could not make/build the dl4j code base using intellij.  for example, i tried to make the deeplearning4j-nn module, and received the follow error message:", "did you follow the buildinglocally guide?", "that gives an end to end description of the setup", "[<-code>]  but i could build the dl4j-examples with no issues.", "you cant just import dl4j", "its a 4 repo project", "well duh", "thats maven central", "big difference", "seems like you skipped reading half of the docs :/", "link  go through this first", "sorry about that.  i thought i could just download the codebase and build it.", "thanks for the information.", "we dont have snapshots yet so you neef yo setup c code", "i appreciate the effort but dl4j isnt just 1 codebase", "wee bit more complex than that", "thanks."], ["hi. i wrote this code to train cnn model on spark using dl4j 1..-beta4. when i run the code in local mode with all my input images in local directory and spark master as local[*] i get 80% accuracy and prediction looks very good. but when i run the exact same code with the exact same input images and parameters, i consistency get 33% accuracy (i have 3 image classes). this is how i preparing my training data:  code . this is my network config:  code . any help explain why the results are different between local and spark based run. thank you."], ["hey, guys after update to 1.-beta5 i have this error:  code . most interesting that i have class in directory", "can you please post your pom.xml in a !gist?", "we had to update the version of gcc, but i think @sshepel still hasn't gotten to update the version of xcode on the ci server, which usually helps with these kinds of errors. please open an issue so we don't forget about that!", "link", "done   issue", "code  you'll need to update the version on that one too", "oh, sorry i see i should use deeplearning4j-ui_ id :1.-beta5 not deeplearning4j-ui_ id  thanks. i will try and let you know", "i updated to deeplearning4j-ui_ id :1.-beta5 but all the same have this error"], ["could you point to an example /doc how to provide dataset with multiple labels  (e.g. recordreaderdatasetiterator ). the dataset has several labels, each can be yes or no (1 /0) , multiple \"yes\" could be set.", "the iterator does that out of the box", "you just specify the number of labels and you're set"], ["hi, i'm trying to import a keras (functional) model into dl4j. i'm using 1.beta7 but have also tried previous version, it gives me an error:  code . for more information, see  link . can anyone help? maybe it has something to do with the architecture or something. i can't figure it out.", "but, the sequential keras models work fine, yet, the functional one, even the ones from the konduit tutorial, don't work.", "there\u2019s a separate api for functional models."], ["i\\'m reading about the rnn stuff and it says \"it is not possible to change the number of examples between calls of rnntimestep\". i'm trying to understand this: to make a prediction, i need the previous state, but if i call rnnclearpreviousstate(), this clears the previous... which now allows me to use any number of examples i want... but because previous state was cleared... this means that i'm back in the beginning? let's say i have daily data, and i do at first 2 examples (days) at a time... after some time i've reached say thursday and could predict friday, but if i do rnnclearpreviousstate(), then i'm back to monday?", "it's pretty uncommon that you want to predict the next steps of some number of time series,and start predicting for a new time series at the same time", "it gets hard to track, too", "in principle: you can manually set the state. it's just arrays, and adding more examples means a larger dimension 0 for the state", "i mean is performance so critical that you really need to do that? if not, i'd suggest keeping them separate", "i'm not yet even trying to optimize, i'm not sure what it is doing or supposed to be doing... for example, if i do single step, then i get next day, right? if i do say 2 examples/days, then i predict the day after tomorrow? or still just tomorrow?", "have you read this?  link  and the javadoc?  link", "that's exactly what i'm reading (not yet the javadoc though)", "so 3 examples would mean tomorrows prediction in 3 locations?", "right, if you feed in an array with size(0) == 3, then that's just a batched prediction for 3 separate examples", "works exactly the same way as a training minibatch or standard forward pass in that respect", "ok, got it now, thanks", "sure, np"], ["does dl4j automatically clear unused nd arrays? because i am working on a genetic algorithm which runs multiple networks at the same time for multiple generations and after a few runs i get a out of memory exception? or is it just the networks overflowing the memory?", "dl4j uses (by default) a workspace memory management strategy. we're not reliant on garbage collection or anything for the activations/gradients (forward/backward pass). so memory use should be flat during training and inference but we are reliant on garbage collection to clean up any network parameters memory (and workspace memory) for networks you've finished with (and have cleared all references to so they can be garbage collected)", "is there a way to make networks more \"lightweight\" because in a ga having multiple gigabytes of memory per network is quite heavy. the network only has 124 params.", "the only way you can get multiple gb of memory use from a tiny network is if you have huge activations (like cnn with like 1000x1000 images or whatever) or huge minibatch size", "i have one network / thread getting fed a 3 x 5 x 5 per game step. anything suspicious ? because they get fed a lot of data but not at the same time. and i am at  id  gb of memory", "and you're sure you are not leaking array references somewhere, or have loaded multiple gb into memory? even if it was somehow from the network (detached arrays - i.e., those not in workspaces) it would be cleaned up by garbage collector. you can check memory use using  code  method too, might show you something useful", "to get the 3x5x5 gamestate i generate 3 5x5 double arrays then i create 3 nd-arrays from those double arrays and stack them onto eachother using vstack to then reshape them into one (1x)3x5x5 nd-array. does this leak ref.? if so whats a better way of doing that?", "that sounds ok. by \"leaking array references\" i mean you have something (like a list, map, etc) that stores lots of arrays permanently so they can't be garbage collected. if you can't work it out, feel free to open an !issue (with a reproducible example we can run) and we'll take a look when we have time"], ["what are the best alternatives to  code  and  code ? those seem to be depracated by deeplearning4j.", "would you be able to tell me if my lstm model is training correctly if i sent i screenshot of my dee2 plearning4j ui? im confused if my graphs are hinting at something wrong or if it is just the type of problem", "if you are interested, here is the link:  link", "it should look more/less the same as training on mnist. if it\u2019s going crazy sometimes it might mean you have some big spike here and there. check that there\u2019s nothing wrong with the data. if the loss isn\u2019t going down try a lower learning rate. other than that not much i can say.", "do you know where i can find mnist?", "mnistdatasetiterator. you should start with that first. but it\u2019s an image dataset so very different. but i mean in general.", "are there any unit tests or methods that i can use to check my datasetiterator?", "nothing automatic. just have to make sure there\u2019s nothing completely crazy in there. previously when i was making a model that used e-mails as input, i noticed a big spike that corresponded to a couple of emails that are completely encoded on base64"], ["i'm having trouble trying to run an example project in eclipse using maven. how should i set the maven project's launch configuration?", "there is nothing to set o_0", "if you are having problems it is likely related to being new to m2eclipse"], ["hi!", "we've tried dl4j with word embedding (word2vec). we found quite convenient to put the original words as metadata in the input dataset. (1) to the original sample classifier, in the datafetcher we added:  code . (2) so when we evaluate the model, we can easily print the input/expected output/actual output:  code . (3) for that to work we had to override the basedatafetched initializecurrfromlist() as it didn't retain the metadatas. perhaps this is a bug?  code . i hope this is what metadata are for. if so, it would be good to patch the basedatafetcher with the missing curr.setexamplemetadata(metas). also, it would be good to have metadata in the dl4j samples.", "thanks for the good work, dl4j people! best,e.", "mind filing an !issue with this feedback?", "done :  issue", "thanks!"], ["hello! just have installed deeplearning4j. i am trying to run nlp examples, word2vecrawtextexample and word2vecuptrainingexample, without any code changes. however i am getting an exception:  code . the reason is method  code . could you please help me to start use of dl4j?  i am using win10; java version \"11\"."], ["hello friends, is it possible to run a java program on google colab?", "i don't think that would be possible", "thank you for reply . hello friends, i have a question i trained my dataset with resnet50 and saved it as 'model.bin' but i have no idea for predict any jpg image, i restored my model as  code  but how can i predict jpg image on this model, can you help me to complete the code?", "by the way i found the solution, thank you for your helping"], ["hello, what happened to this page?", "there has been work to update the documents and that page was pulled. how did you get that link?", "that page was out of date, what page linked you to that?", "also, what did you need to know about our architecture?", "@tomthetraineri just used the search engine and a link provided in a paper. what i was trying to figure out is the parallelization strategies you are using", "what is the paper you were looking at?", "it's from a double blind review so i cannot disclose it, sorry about that", "no problem - more just trying to understand where these links still exist", "sure", "regarding the parallelization strategy, we apply the batch training approach where we divide the gradient updates by the batch size", "thank you.", "i am trying to figure out those strategies given that i am new to neural networks but not new to parallel programming", "are you wondering about parallelism model for computations then?", "i think that's what i am looking for. it would be nice to get the idea of how you carry on this computation, reduce, and so on"], ["hello @agibsonccc , i'm getting some jvm crash under specific conditions  code . i believe i should report it. this code  code  leads to crash like this  code  . note that the output of the system out will have the same shapes, not the reduced one that is expected at the first system  feature shape  code while my expectation is to have feature shape  code  at the first row.", "issue  raised this one.", "and also this kind of stuff occurs, not clear what makes it process finished with exit code. but before i have a warning like this:  code ."], ["how can i make read data format .h5 (hdf5) into datasetiterator? the data structure of my .h5 file is like this directory  i can read it easily and assign it to ndarray but i dont how to assign it to datasetiterator.", "you can use the new javacpp based presets  link", "you would use that with dataset.load. if you can do a chain of saves in to a file this should work. what i would maybe consider doing if you can would be prepending an int and reading that in to represent the count of the number of datasets. then you can just chain save and load calls", "okay let me clarify your answer. you suggest me to chuck my own data without using datasetiterator right?", "i never use that javacpp before. it probably takes me some time to really get what you suggest.", "it's just java bindings for the c++ hdf5, same api", "your goal is to basically get an input stream and an outputstream that you can write and read raw bytes from", "i just told you the binary format to use from there"], ["anyone, implemented the word2vec and plot in a tsne graph .", "link  this is how you save tsne coordinates given a vector", "link  shows how our ui works", "actually looking at the new ui..it looks we may need to implement the new tsne yet. mind filing an !issue?", ",  i ran the former example before, but when i ran the later example, it consumes too much resource and my system stop responding. thank you,"], ["hello! i'm trying to visualize the model training. but on  link  i don't see any changes, just blank sections for graphs without any useful info; all i was able to see is diagram with layers on directory but again without any graphs. here's how i setup listener: code  and this dependency is in my pom.xml:  code  .", "sorry if you want faster replies, please go over to our forums:  link  . we only check this once in a while now due to people asking repeat questions in the chat. the forum is the primary channel so people can search for answers in the future."], ["@alexdblack, the save model function doesn't seem to work for this library. i tried code from both the example github library and the video, yet it cannot find the file. i've tried both  code , come up with same error:  code  . any help would be appreciated. thanks!"], ["hi, i'm trying to use opencv within my dl4j code. i ran the basic example helloccv. i get the following error (missing a library)  code .", "i forgot a call to loader.load(opencv_java.class)..."], ["hello. i am trying to import a keras model into dl4j. i have managed to go through some layers but one of them is giving me trouble. this is the error i get  code . what does this mean and what can i do about it? thank you for help.", "@vent1narc file an !issue please, and include your model there."], ["i am trying to look into dl4j core source so i figure out how to use uiconnectioninfo. mvn download sources does not seem to work. is dl4j core source available? it would be helpful if someone at least put up a example that displays a plot on uisever from beginning to end. otherwise, its time to give up", "i\u2019ve posted you a link few hours ago", "link to repo   link  and download sources work as well", "it\u2019s kinda requirement", "ok, thank you", "it\u2019s right there at maven"], ["can someone give me a pointer to how i link a signed eclipse agreement to a pull? i have a pull request which fails checking.", "commit locally by signing with commit -s (after setting up pgp and your signature), then push to github. your github account also needs to sign some eclipse agreements.", "i'll try that. my lawyers have just approved the eclipse agreement i have a pgp signature (my lawyers don't know about that). thanks. tried & failed", "does signing the commit work? the 'git commit -s'.", "it did. but still failed the checks", "what is the error you get? (maybe a !gist if it is a long one)", "it's not an error during upload (to my fork). it's when i do a pull - get a fail on eclipse thing", "eclipse hopefully working on this. i remember spending some quality time setting this up. does the fork show the pr as signed?", "pr ?", "open the pr anyway, let's see what's wrong", "pull request. from your fork, you create a pr (pull request) to the eclipse repository.", "i redid open a pull request (pr). only 1 file changed. if you chaps can copy it - it will work", "#8187 lacks the eclipse agreements.", "i saw that. i had signed the eclipse thing on their website. but no idea how to connect that to the pull. ok - so the first commits were not authorised? i added eclipse legal thing after it failed", "ok - give me a while", "try a simple one first. just add a \"hello\" line, sign, submit (and close!) just to see if the checks pass.", "omg - when did lawyers get into all this. thanks both (   ) of you chaps for the insight.", "not having much luck"], ["@alexdblack i'm getting the below exception while running the spark submit:  link . i may have not allotted enough memory for the operation. but i guess that is not the root cause here. code:  link  hdfs having pre-processed data:  link   link . any idea whats the root cause here? this is the only similar reference found on web with this error:  issue", "i tried both 1.-beta3 & 1.-beta4 but i still get the same exception"], ["is there a way to use a deconvolution2d layer as an outputlayer? i'm trying to make a convolutional autoencoder", "just add a losslayer at the end. @sathyamoorthyrr also what version of sbt are you using?", "i now have a new problem. deconvolution2d doesn't like to take input from a denselayer.  link  how should one reconcile this? is this what preprocessors are for?", "yeah. need to reshape the data to be square.", "would i add the preprocessor between the layers, and would i add it like i would a layer? feedforwardtocnnpreprocessor?", "you add it in the end with setinputpreprocessors. the examples show you how. ... i\u2019m sure there\u2019s an example somewhere that uses them. one sec.", "link   link  like this."], ["um, not sure if this really is an issue or just a small bug, but i was running the graveslstmcharmodellingexample, and while it was running it prints out the samples of text from shakespears work, but it seems to also include the headers or footers of the downloaded file. as some the examples have:  link . its not the biggest issue or bug in the world, but i though i would mension it."], ["hi, i've just migrated my simple project from dp4j- id  to dp4j-1.-beta5. and got an exception as:  code .", "any hint? thank you. my env:  code  java version:  id", "code  the exception thrown by sentence:  code . i've googled, there is no immediate solution. while there is similar questions asked and the seemed that the problem was caused by some dependency version conflicts. but i don't know where to go further. the class threw the exception is just a copy of lenetmnistexample."], ["hello, i just upgraded from dl4j-beta6 to beta7. wordvectorserializer is taking too long (2+ hours) to load golve300 dimension file and it errors out finally. just trying to understand if there is some regression issue happened in beta7. are there any quick fixes available? i'm using jdk 8, windows 10,1.-beta7. here's the exception stack  code . if i move back to beta6, it works. this happens only in beta7 and for other glove files as well.", "hello, i made some work on wordvectorserializer in beta7. can please you tell me, which method from wordvectorserializer do you call to load the model ?", "hi alexei, thanks for the quick response. i was using   code  .", "can you try to load model using  code or  code  ?", "issue  this can be related to your problem.", "the glove file i use is glove.6b.300d.txt which is a text file. i tried both  code  and  code , both threw the exception  code . to resolve the numberformatexception, i had to add the wordcount and the dimension , even then it fails subsequently. tried  code  and it's running for hours. i don't see any out of memory error. i tried few other methods on the wordvectorserializer. it either fails or runs for hours. per the commit history, it looks like support for glove was removed. am i right?", "better to ask this question directly to author of this commit. i was working to make model loading more flexible on beta7. but i not specialy touched to glove"], ["how do i know if i have nd4j backend on my classpath? i get an ndj4j backend error when i try to run  code  where  code  is my  code  class.", "please post your pom.xml using !gist", "you could also refer to these: [<-url>] [<url>]", "hi. i have open a issue at dl4j  issue .", "thanks! the engineers will look into i\\."], ["hi, i'm trying to import a tensorflow .pb model. i'm getting a runtimeexception error  code . i'm using this model with python and it gives me results.", "it looks like it is model dependent.", "hard to say without the model. but that doesn't look like any issue we know about... are you able to share the model? if so, open an issue and i'd be happy to take a look"], ["hello,", "when i run the dbnfullexample by use intellij idea, and i have the following problem [a fatal error has been detected by the java runtime environment:  code . i try the following command in terminal  code  then rerun the program but the problem not solved . what should i do to solve it?", "could you file an issue on   link  and give us literally everything you can about the crash? os, how you ran it, version of java include that crash log in a github gist as well  link  as well in another github gist", "the os is ubuntu  id  lts and the java version is java version \" id _101\" openjdk runtime environment (icedtea id ) (7u101- id -0ubuntu id ) openjdk 64-bit server vm (build  id -b01, mixed mode)", "github. issue. i'm not here to dig in to your issue this millisecond. please do exactly and only as i say. nothing more or less. none of that matters right now. we will get to it when we get time. like i said we also need that log and everything else in a github issue only not this chat", "ok , i will do", "thanks"], ["does anyone know if there's a way to allow the building of nd4j linux-x86_64 cuda backend jar files on macosx? long story short the 'magical' dependency generation for platforms keeps trying to make nd4j-cuda- id :1.-beta7:linux-x86_64 depend on nd4j-cuda- id :1.-beta7:macosx-x86_64. i'm in a fairly constrained environment where maven deps are auto converted into a bazel build file so i'm limited in what i can actually change", "mind switching to the class forum so people can discover search results easier later? tldr; use  code  and i believe it'll work. the \"magical dependencies\" have docs here you can read; more here:  link", "thanks adam, will take the conversation over there."], ["this question may be inappropriate. in my string representation for music, i use, for example, ascii char 33 to represent a duration of one beat, 34 = two beats, 35 = three beats, etc. will lstm take advantage of the numeric proximity of those characters in its learning? or does it treat the various characters as unordered?", "my guess is that the one hot input is essentially unordered to the internal layers. each one gets mapped to a unique weight in the first layer.", "with a computationgraph i could wire input duration nodes to later layers, right? any good examples of something like that?", "i'm surprised there are no gan implementations in dl4j.", "i have one  link .but haven\u2019t looked at it in a long time.", "will transformers be added?", "we have self attention layers. i do believe we have everything you need to make one."], ["hi. i'm noticing i'm get different training results with same seed value. does the training config respect the seed value?", "it should if the data is the same. if you use nd4j.rand (or use something that uses it) you\u2019ll want to also set that seed. with  code (or another seed)", "my code is below:  code . i'm getting a variance in f1 scores when i train/test with the same data.", "hmm. that shouldn\u2019t happen. how do you get this data?", "i'm using the evaluator to generate the f1 scores  code . i just ran a test with the cpu, i'm seeing the cpu results are different than the gpu. the cpu results are all the same (as expected) but the gpu results have the variance. i'm using deeplearning4j-cuda-1 for the gpu.", "i'm running training in multiple threads.  each thread trains a different net.", "cpu and gpu results are typically slightly different. but the gpu should be the same across runs.", "so if you have multiple threads, then you\u2019ll have a nondeterministic order somewhere. but why are you training from multiple threads?", "i have a messaging system that dispatches train requests to multiple threads. however, i just tried training with one thread and i'm still seeing the same issue.", "so if the data is definitely the same, and coming in the same order and the cpu version works correctly there might be an issue we have to fix. (gotta love these surprises nvidia like to sprinkle in their code.)", "i'm running ubuntu id and cuda version  id  on a 1080ti", "huh, i expect the 1080ti to be well behaved. is it possible to get a minimal example showing the problem posted as an issue?", "ok let me work on that. thank you for your support.", "i just tried it in beta3. everything is correct in beta3 on both gpu and cpu but in beta4 the results are different on gpu. should i upgrade the cuda to  id ?", "that shouldn\u2019t change anything\u2026. i wonder if it happens in snapshot. beta5 is mostly cuda fixes.", "okay its working correctly in the 1.-snapshot. any way to get a stable version of this we can use for development?", "it\u2019s best to wait soon. it\u2019s on it\u2019s way out the door."], ["hi, can someone give me a little tip how i can use quotes on the csvsequencerecordreader. nothing here in the constructor like in the csvrecordreader.", "it\u2019s just a small utility class and not as complete as csvrecordreader. a pull request adding that (based off of the csvrecordreader implementation) would be really helpful to everyone.", "thanks for your answer, i solved it in first step with base64 encoding. and decode inside the columntransform. for a pull request i will try to checkout and build first never done before", "cool."], ["hi all. i have a mac and use maven. in my pom i added:  code . i try to call the word2vec class but i get that the class is not identify (attached). can anyone help? note that when i use my windows computer, everything works fine", "use nd4j-native-platform and ensure intelij/eclipse updates its cache properly", "wow! perfect! thanks!"], ["hi, i'm trying to make image segmentation neural network (detect pipes on photos; same case like finding roads). i was trying to find something about this, but i can't. one thing i found is  issue , but code isn't complete. base on above i have tried to write my own network, but something is wrong, because: for my input image - 3 channels 416x416 output is 1 channel (what is expected) but size is 101x101 (should be 416x416). ignoring above invalid size (i have scaled output to 101x101) the response from trained network is also invalid (output is always  id  for every pixel). i think i have wrong network config and i would appreciate any help with runing it. my current code is:  link", "may be you can add a statslistener to watch how fitting is working. see  link ( a bit out of date with 1.-beta6).", "i had in my code statslistener, but this doesn't help. my main problem is that i'm beginner with deep learning and i don't know what layer is for. there for i don't know how to configure network for such a task. i have read a lot of articles for this kind of problem, but with my current experience is hard to move this knowledge to dl4j. i'm pretty sure that my config is bad, because i wanted to have output from network same size like input (416x416), but i don't know how to accomplish that.3", "well, statslistener is helpful in showing graph of loss function (it should be a decreasing curve) and weights updates. you will be able to detect gradient explosion. but i guess you problem is with the model you are using. why not try to use an already existing model for this task, such as unet from dl4j's zoo ?", "thank you for anwers. i will go with unet. i missed this."], ["link . hi, my computer config is i3 cpu, and 8g ram. but when i run this example of  word2vecsentimentrnn, it keeps in this state for a long time. does that mean my computer config not enough?", "yes", "ram isn\u2019t enough", "\"i3\"?", "nvm cpu, ram is an issue here", "yes", "that will take forever to run anythin g:d", "ram too though", "i bet pc is swapping", "i asked u before, and you said you have run this in your lap, and it's 8g too ,hh", "well it\\'s \"possible\"", "doesn't mean it's ideal", "adam's laptop is slightly more then 8g", "my travel laptop is 24gb", "ok, maybe i should get a better pc"], ["good morning (again!) trying to run arbiter and trying to figure out how to create the  code ; but this is not splitting the dataset correctly. how to i create a  dataprovider from a recordreaderdatasetiterator?", "you'd want to split the data yourself", "link  if you pre save the datasets you can use this. there's also balance minibatches:  link", "can i use splittestandtrain?", "not really, unless you have the whole dataset in memory", "you could go through the iterator and randomly do split test train and save the results", "when training my model, i use  splittestandtrain on a recordreaderdatasetiterator (after applying normalizerminmaxscaler) to load the data. does this not work?", "well so again, you till have the problem of not having the whole dataset in memory", "you could do split test and train on the batches and concatneate", "my batch size is the entire data set so i should be fine", "then yeah split test and train will work"], ["hi! training an own yolo2 model, is that difficult if i want to have more than 5 classes? difficult = large computer, lots of pictures(1000 per each class). or is it better to use a pretrained yolo2 model with dl4j if i don't have a good computer?", "the number of classes doesn\u2019t really change how fast/slow the model is. (within similar order of magnitude)", "so using less classes, requires less pictures too per each object? e.g if i have 20 classes. then i need e.g 1000 pictures of each class. but if i want 5 classes. then i need e.g 50 pictures of each class?  will dl4j have yolo5 in the future?", "ah. in that sense you probably want just as many for each class not less. re: yolo5 pull requests accepted. i might add my yolo3 implementation that i have next chance i get. from there i think it\u2019s not too big of a leap to add yolo5", "great! i'm building a web application with dl4j + vaadin where i want to connect a camera with the computer. but i still wondering if i should use a pretrained model, or i should build a model by my self. i have no experience with deep learning by the way. i know how a simple vanilla mlp network works only. if it to difficult to train a complex yolo-model, then i think i will use a pretrained model.", "pretrained is good for this kind of demo. then once it\u2019s working it\u2019s easy to train a different model to suit your needs."], ["hello, i'm a new learner with dl4j , encounter a performance issue when using parallelinference with multiple threading for cnn model. the cpu load and until got very high when using 20 threads to call it. dl4j version is 1.-beta3, and my code is like:  code .", "can someone help to check this? thanks!", "first - upgrade to 1.-beta5, that's the current version. second - i'm not sure what the problem is... too low or too high cpu usage?", "hi, thanks for reply. the issue is too high cpu usage and load. the model i'm using was trained is based on 1.-beta3, if change to 1.-beta5, need retrain the model. is there some guidance for setting up parameters in parallelinference, for example the bat4 chlimit and number of workers, which number i should set it the machine used is a 2 cpus (12 cores)?"], ["hi i use autoencoder to do feature processing. i get the model. but i find the input vector[00001,  id ,  id ,  id , , , ,  id ,  id ] is converted [0,  id ,  id ,  id , 0, 0, 0,  id ,  id  ]by this method indarray features = nd4j.create(featuredata);if i want to keep more decimal what should i do?", "it's just formatting on tostring check with getdouble if you want", "ok thanks, i will try"], ["hello everyone, i'm currently facing an issue using spark and dl4j. i'm trying to fit a neural network using spark and to debug my training. i've attached a statslistener to my model to send training stats to the ui. the server ui is in a different jvm as advised in the tutorial when running spark applications. the problem is that when i use the sharedtrainingmaster, no information is sent to the ui. however, it works with the parameteraveragingmaster implementation. this is my code :  code . after some digging, it seems that my issue is the exact duplicate of this :  issue  which was solved a year ago. i precise that i don't have this problem when i run spark in standalone mode. only when i am in cluster/yarn mode. does anyone have an hint on this ? thanks in advance", "what dl4j version are you on? if it's not 1.-beta4 or (better yet) 1.-beta5, try upgrading. otherwise - open an !issue and we'll take a look"], ["the autoencoder example is quite simple... so i have still plenty of questions. first of all, there are many types of autoencoders such as denoising autoencoder, sparse autoencoder, variational autoencoder, contractive autoencoder... which does dl4j support?do i need to do something special to make an autoencoder that is  denoising autoencoder, sparse autoencoder, variational autoencoder or contractive autoencoder?", "variational autoencoder now available at current master", "alex merged it few days ago", "nice!", "there are small special things for each of the subtypes of autoencoders.  sparse has a penalty (usually l1) which encourages sparse activation in the inner-most hidden layer.  denoising adds noise on the input and attempts to reconstruct a noise free output.  variational autoencoders instead learn a mean and standard deviation instead of an explicit internal state, then randomly generate a hidden state from those values and reconstruct.", "of the bunch, denoising is perhaps the easiest to train, since you just had to do train(myexamples + noise, myexamples)", "i know the differences in the autoencoders. just wondering how to implement them in dl4j (for example the l1 stuff that you mentioned)", "ah.  sry.  i misunderstood.", "in this respect, i can't really help.  i'm here for the nd4j stuff.", "i mean, i know, but i don't know the exact math behind them", "'tiz ok"], ["within class lenetmnist example in deeplearning4j example package, when you train the model, how can you specify the input data label to fit the model? i only see you using model.fit(inputdata) without specific the label with it.", "dataset contains input and label fields", "but if have separate dataset how can i combine them, for example by dataset input is 3 dimension and output is 1 dimensional output", "based on this link  link  , can i just declare it like this dataset input = new dataset(input,label) while both input and label are ndarray with different dimension", "input and label fields can be any indarrays (that are appropriate for the network you are training, anyway)", "and yes, you create it using that constructor", "thank"], ["hello i'm new in nn, so i have problems with my csv file, i don't know how to transform text(or remove from csv file) to the binary. i have 2 csv files train and test. in train 81 column in test 80.example of csv:", "you'll need to convert that to all numerical values before you can feed it into a neural network. for example one-hot encoding for the categorical values. one approach is to use datavec, but there are other things you can do too  link", "thank you i will try."], ["are there dl4j resources for loading time-series data stored in hdfs and constructing an iterator out of it to fit a model?", "as in, locally fitting from remotely stored data? yes, it's similar to a local data pipeline (for like csv or whatever) but you should use the following inputsplit instead of say fileinputsplit:  link", "thanks for the reply. but i was referring to data in hdfs and using within the cluster on something like zeppelin. would the information you provided still be the same?", "right, it should be the same in that case. there's basically 2 cases here (a) cluster execution - apache spark etc (b) single node execution - accessing remote data. i described how to do (b)"], ["does anyone know what activation function goes in the output layer of an rnn for a sequence to sequence architecture. i am forecasting but it seems dl4j does not have a linear activation function... would relu work?", "seq2seq doesn't imply a specific activation function. for discrete outputs, it's usually softmax (as in any standard classifier), for regression (real valued outputs) it's often identity as for \"linear\" - you're looking for identity activation function (i.e.,  code )", "thank you, just what i was looking for"], ["i am trying to upgrade from beta2 to beta4. and getting  code . running tomcat 8 with java  id . beta4 seems to have changed dependencies and i'm getting this on a large number of dependent jars. does beta4 expect to be able to run under java  id ?", "logged as an issue  issue"], ["hi. when i try to install nd4j on an arm64 server that doesn't have glibc  id  i get the error:  code . however it does work when installed on a server with that glibc. is there anyway to support other glibc environments? or is everyone who doesn't have glibc  id  considered unsupported? thanks, greg", "bump for my post", "are you using a really new os?", "ubuntu 16. i have  id  installed and it needs  id . what would be the best way to resolve this do you think?", "ah, the opposite problem. for ubuntu\u2026 not sure.", "what are the general steps for an os you are familiar with?", "in red-hot/centos there\u2019s devtoolset that you can possibly install. and you run it after you\u2019ve activated a recent enough devtoolset.", "ah cool. can it also do downgrading if needed?", "otherwise, you can create a chroot with a newer version. you have to \u201cactivate it\u201d every time you want to use it.", "ah okay good to know thanks", "same goes for the chroot."], ["i try to use  matlabrecordreader but i can't understand how it works, anybody can explain me please?"], ["hi, i have an issue with an big enterprise application that is build on windows an run on linux in a docker container.", "i tried to reproduce it with a very simple example. i used the standalone example from the dl4j github repo and build it on windows. than i copied the jar to linux and tried to execute the example with leads to an error:  code . so i tried by adding the linux nd4j dependency explicitly to my windows build using an additional dependency section in my pom file:  code . this in turn leads to a new error:  code . i'm a bit lost here. is it supported to build an application with maven install on windows and deploy to linux?", "use  code , not just  code", "thanks a lot, that solved my issue. feel a bit stupid."], ["hi, i am using pca of nd4j to reduce the feature dimensions, however, i am not sure whether i am using the api's/methods properly. could someone provide me an example to use the pca of nd4j. i am not able to find any examples of pca here:  link"], ["is it possible to speed up dl4j on android and iphone using cuda or something else? i have tried to train a denselayer+outputlayer with gluon's javafx and dl4j and it's working. but i tried on my samsung s5 neo and it was quite slow, the phone. yes, my android app(deeplearning2c) is done and ready to use. but i want to improve the speed. suggestions?", "i've been using the accelerate framework on ios, and that was really a performance booster (but it's a year ago i did that)", "i'll fork your code once we have the graalvm integration complete, this will be a nice performance test and demo.", "as far i know, there is no cuda functionality for mobile as most phones are arm-based processors. you can try tweaking the vm heap size of your device  link", "that's sounds nice. is it possible to create csv files from images in dl4j?", "i have some ideas about add another page inside dl2c that converts images to csv files", "i recently wrote a post highlighting dl4j, it's a fairly simple one but a good starting point for many new to these topics - see  link   what is represented in these csv files?  thanks for sharing!", "numbers only. my goal is to create a tool that simplifies the use of dl4j. so i created deeplearning2c. available on my github. it's a c-code generator that works for booth android/iphone and directory  thanks to gluonhq framework. but it can serve as a regular dl application as well. the c-code generation is just a special feature. in the future i want to implement c-code generation for lstm-networks too. right now i want to improve deeplearning2c by adding a new page in that application. the page need to have graphical ui:s that allows the user to e.g select a folder with images, determine what resolution they are going to have, generate a complete csv file for classification. ready to use. simple and easy.", "right now i'm developing on new application that can generate regression csv for dl4j/dl2c. it's called jwifilogger. for booth android/iphone + desktop. soon available on my git."], ["how can i dynamically adjust weightdecay or l2 in a running multilayernetwork? i don't see setters for those.", "same with dropout.", "according to  link  , ya can't change l2.", "you can always make a new network and copy over all the parameters.", "fyi, i made javafx-based visualization classes for dl4j that use a traininglistener and associated learningschedule to let you dynamically control the learning rate and learning rate decay of a running network, while visualizing the layer stats  link .", "here's a javafx ui for dl4j. it allows you to adjust the learning rate via the keyboard, as well as the decay factor of the learningschedule. it shows learning stats (ratios, etc) and a linechart of scores. you can zoom in and you can smooth the scores  link", "nice."], ["hello, in mnist for experts example i can see pretrain called on builder. what kind of pretraining will be used if it is set to true", "pretraining is special option for autoencoders/rbms", "can i use it to pretrain conv net weights using rbm automatically?", "i should be clearer: i want to do pretraining using rbm and then transfer the weights to conv net before starting backprop. will builder.pretrain(true) do that for me?"], ["hi, i want to build the maven projet  link  on windows 10. any help for this ?", "i guess you need to have java, git and maven installed. extract projects from repositories. and follow instructions.", "on windows i think that i have to install bash", "no, you do not need bash. you can retrieve projects using github desktop if you're not familiar with git. (or you are talking about git bash)", "github desktop ( link ), maven ( link ), git ( link ), java ( link )", "yes i know but to build you have to put :  code"], ["hi how to run deeplearning4j on hadoop by using mapreduce? is there any examples?", "there\u2019s none, mr isn\u2019t supported anymore", "spark is the way to go", "we had it at 1 point but no one was willing to pay for it", "it was supported earlier, but ^", "we have salaried engineers working on dl4j", "if something is a maintenance burden we cut it", "we did that with spark.ml too", "same thing", "maintaining 2 versions of spark wasn't good for us", "ok, i will switch to spark too"], ["hello, question: we run into an error when trying to run the emnist example with macos catalina & java 8. but this throws an error 'outside the jvm', very comparable to this issue:  issue  is this known/fixed? oh i see that it's the same issue as  issue ."], ["hi, i am facing the following error one i trying to run rl4j-examples", "error: code  i already have installed android sdk and intellij already have path configured", "try to run one of the examples for android first", "thanks  i will try"], ["hi, how can i avoid score value being nan during training? i think reporting nan is undesired. am i correct?", "!tuning", "!tuning", "hmmm", "if you want help from us prove to me your read our tuning docs first", "ok", "go through those steps to your hyper parameters and what you've tried", ":)", "ok"], ["when i call  code  throws that exception:  code . my code :  code  on beta4 was working.", "code   code  ?", "just use  code  if you want the last dimension."], ["hello! i'm having an idea. just as deeplearning2c(going to update that project soon), i want to create another project in c by using dl4j. my idea is that implement reinforcement learning onto a microcontroller such as stm32 controllers and it's should be very adaptive like \"plug 'n' play\". is that possible, or does reinforcement learning requires lots of hours of tuning? i'm talking about simple examples here. simple as control engineering.", "anything simple is going to require much processing power for learning, yes", "thank you for your reply. perhaps it not worth the idea to starting to create this.", "or if i'm using like a raspberry pi. then it might be possible?"], ["hi guys, short question our nexus has problems downloading  code", "has anyone else had this problem?", "how is this related to dl4j ? this channel is for dl4j/nd4j support", "that library is available on mvnrepository. i\u2019m guessing it\u2019s a nework issue on your side. also, like raver said. not really relevant for this channel"], ["hey guys, can anybody explain me what is going on here with my gradients?", "maybe you can adjust the batch size", "when this happens, this is what happens to the score.  would you go higher or lower? could this be a vanishing gradient problem?", "i\u2019d double check the data.", "it does happen always at the same data interval. if i restart the training it happens at another timestamp  code  made it a bit better", "i will go a bit higher because before this when i try to train my model the same problems happens to my model", "thanks i will try that. i just discovered this. i guess that this is not normal, too. this is my lstm layer", "does noone has an idea what is going on here? i tried different batch sizes and the problem remains. i checked the data but i was not able to find any nan or other uncommon things. maybe some outliners in the data, but could that lead to such a behaviour?", "yeah. one before i was doing e-mail processing, a few of the e-mails had huge sections of base64 encoded files in-line. caused huge spikes in the loss.", "so i guess i need to do some outlier removal :/", "@nicoladaoud hard to tell what it is, you likely have a bad set of deps. maybe use nd4j-native-platform instead?"], ["hello! i am trying to import numpy arrays to nd4j, how can i do that? i saw jumpy which i hope and i save from and load with nd4j, but is there a documented procedure for that?", "there's a few methods you can use  code  methods (for text-based numpy format - np.savetxt i think the method is)", "thank you! and just so i can be more independent, is there a readthedocs kind of site where i could normally get that info?", "code  throws a segmentation fault on my machine:  link", "core dump itself:  link . please tell me if this is worth opening an issue", "well, that's unexpected. it's pretty well tested... can you share the file you're loading, and more details in an !issue? thanks"], ["hey guys! is there any information around the byte size of an indarray given the shape and datatype? ie how much memory would a float indarray of shape 10x10 take up?", "as far as the actual off-heap memory, it's just array length (i.e., number of elements, product( code ) times size in bytes of each element. each element will be their standard sizes - so 4 bytes for int/float, 8 bytes for long/double, and so on. there's a little bit of overhead on the java side (objects etc), plus the shape buffer (small, immutable, and shared between other arrays of same shape/strides etc) but for large arrays the buffer (i.e., actual array data) takes up the vast majority of the space", "thank you!"], ["i have a question about how nd4j find the location of native .so files.  all native .so files are packaged into a nd4j-native- id -linux-x86-64.jar file. but when javacpp loads .so libraries, normally it needs class to find the .so libraries. but i didn't find this setting.  so i wonder how javacpp find the location of native .so files.", "it's based on the classpath", "javacpp has a loader class", "but when i was trying a simple example of javacpp, it said unsatisfiedlinkerror. i have to set ld_library_path to suppress it. i have the .so file on my classpath.", "alexdblack: i figured it out:", "i need to include platform name in the directory in the jar file that contains the .so files.", "thanks for pointing me to the source code of javacpp", "sorry, @ the wrong person."], ["i spotted a pretrain boolean parameter in configuration. does this turn on greedy layer-wise unsupervised pretraining for directory"], ["i want to take compressed output from hidden layer of autoencoder.some one suggested to use feedforward method for that but i'm not able to implement that.pls help", "literally theres' nothing else to do other than use feedforward... that gives you activations, which is exactly what you are asking for", "code"], ["hello everyone! does anybody succeed in running any example provided with rl4j on a 64-bit ubuntu running in vmware player? i tried several days but cannot get it work. i traced the exception and thought it may relate to use of some android sdk which is required by deeplearning4j but seems not able to work without android device. thanks for any response. i run the examples within intellij idea", "what is the exception you are getting? mind posting it in a !gist?", "thanks for reply. i am on my way to office and will post the exception later.", "when i try to run  code  i got the following error:  code", "i have posted a gist for this issue. it is grateful for any suggestion for us to get rl4j work on our machine. thanks a lot."], ["what classification algorithm from the dl4j-library would you recommend for a case with ca 20 features (mixed categorical and doubles) and either label 0 or 1?", "usually for that you would use a multi-task network (computation graph with multiple output layers). one output layer configured for regression (the doubles), and one (or more) for the categorical values", "thanks! does it exist any examples that could be of use for me?", "for the graph structure, take a look at this:  link"], ["hi i am new in dl4j..how should i have to start to learn this.. i have exp in java using spring and hibernate", "start with examples, depending on the task you're aiming at  link"], ["guys help please. i don't understand what each gate in lstm architecture produce. is it a number or a vector? if i'm not wrong  sigmoid function always produces only one number in range from 0 to 1. or it is possible that every gate produces target vector that was obtained by applying sigmoid function to each element of  input vector?", "each gate produces a single number in range 0 to 1. you have 4 gates per lstm unit, and multiple units per lstm layer", "is number of units == lstm.nout(numberunits) of layer actually?", "so every lstm block produces number and not a vector?", "yes", "by lstm block i mean lstm unit", "yes, it's the same as any other rnn (or, any other network in general) - each unit gives a single output/number", "it's all vectorized for implementation though, across all units", "really thank you! everything became so clear"], ["hi, there. i am new to dl4j and trying to run it on an spark on yarn cluster. is there any configuration to do with the cluster? i cannot find any document about it.", "link   not sure how you missed this", "we mention yarn and the like right in there", "including the memory configuration", "our !examples have spark in them as well", "thanks.", "i did notice that page but only find the guidance about application programming. i mean what exactly i need to do to give the cluster the right environment for running dl4j on spark? such as the dependencies issue.", "it's just a spark job really, it's nothing different than what you already do. dl4j-spark is all you need", "you can look at our examples for that stuff"], ["hi there. i am new to nd4j and am trying to get to grips with the csr sparse matrix ( code ). i have created one as an indarray and would like to print it out to ensure i have done it correctly. however, the normal  code  just prints  code . also  code  and   code  return only zeroes. any tips?", "(the deeplearning4j-examples project sadly does not cover createsparsecsr or createsparsecoo)", "ah - i have answered my own question - you need to call  code  first to return a regular indarray on which you can perform normal operations."], ["hello! i'm new to deeplearning4j, and now is trying to load vgg16 model from local filesystem. but i encountered a problem that jvm throws outofmemoryerror when i calling  code . how much memory does the model actually needed?", "sorry if you want faster replies, please go over to our forums:  link  - we only check this once in a while now due to people asking repeat questions in the chat. the forum is the primary channel so people can search for answers in the future.  post over on the forum with more details about how you're trying to run it. it's hard to tell from just the model you're trying to run.", "thanks"], ["hey! always if i use a random in a filesplit the analyzelocal.analyze throws a class  code . without the random everything works as presumed. anyone an idea?", "after reading linerecordreader.java, analyzelocal.java and filesplit.java, i think you are experiencing a bug. you can file an issue in github. you are using filesplit with one file, so when  code  is called on filesplit, iterationorder is not initialized. it is null, hence the npe."], ["hello everyone !", "im also new to dl4j, tried some stuff, read about it and really like it . but what im currently struggling with is how to enable multi label data input in dl4j (an useful example would be tag prediction from an image) . for inputs with a single label i used the pathlabelgenerator, which was fine. since google and reading the docs did not help much (apart from redirecting to gitter ), i decided to ask here.", "funny, someone else asked a very similar question a while ago about predicting multiple classes", "i think the only way is to take the n classes with the highest probabilities after the softmax step", "maybe have a threshold on minimum probability and take all the classes with at least that probability", "but i take it you want to assign multiple labels to examples during training time. no idea how that would work honestly.", "except maybe copying the examples that have multiple labels, one copy for every label you want to associate it with", "do you mean using a pathlabelgenerator and having a path for each of the labels?", "i can't think a clean way to do that with the current built-in functionality.", "open a github issue in datavec with your use case (i.e., where the labels are coming from - flat csv? map<uri,?>, etc) and we'll see what we can do,  link", "ok thanks for the fast replies!"], ["hey everyone! i am trying to deploy a model in a spark-cluster through spark-submit and am running into an error i believe is related to my build.sbt and dependencies management. i am building an assembly-jar but at the moment my spark-job throws the following exception when creating an indarray  code . i am mainly including:  code . i am in scala id and spark id . i suspect i need to add some more dependencies where it is explicit that the nd4j backend is only cpu. i am wondering if you have a working build.sbt at hand with dependencies versions for this spark+dl4j+nd4j+scala situation i am."], ["hi, i am also new with deeplearning4j. i wanted to try out rl4j a3c too, but i have a issue with sbt. when i run sbt compile i get the following error  code . i saw this issue in gitter and as github issue, but none of the proposed solutions seems to work for me. i tried out sbt versions id , id , id , id , id and dl4j beta 4 and 5. do you have any idea how to fix this? thanks a lot in advance!", "ok nvm, i just tried it again and now the solution with adding  code  is working for me. i am pretty sure i tried that before, but ok. maybe the reboot did the job. thanks anyway!"], ["i want to use lstm to predicit the passengers , i've download singletimestepregression demo, but the passengers_train_0.csv i not found it, where can download it ? hello, everyone. who can help me ?", "which example is this?", "link   link", "where did you find this example?", "these are the error and train.csv. i download on this url: link", "i would like to use bidirection lstm to forecast stocks. where can i download similar exam6 ples  deeplearning doesn't have similar examples. i found it some days. cay you help me ?", "not sure what happened to that example.", "thank you. the examples provided by the official website are too few and too simple to imitate", "maybe look at this one  link ?", "it's not good for beginners to learn", "the examples want to be simple so that it\u2019s easy for big inners to learn.", "ok, thank you. this website have chinese language?", "i thought we did....", "such as qq group", "but look at bewithme's blogs. they are good  link", "thank you very much"], ["i'm having some weird behavior with the nd4j rng. when i generate sequences of numbers, the generated numbers are not the same even though the seed and position is the same (i literally call setseed(0) right before generating them. for example, here are some outputs: run 1:  code  run 2:  code . i can't reproduce this in a smaller program, only in my actual model. any idea what could be causing this? there is no asynchronous stuff happening either"], ["hello! i have an issue i don't know how to solve. im declaring csv file reader with csvrecordreader and trying to pass it to recordreaderdatasetiterator, i have 10 features and 11th element is continues value and iterator asks for number of possible labels. how do i solve this ?", "i've figured it out, you have to add true at the end for it to indicate regression...", "yes, that is correct. i missed your question."], ["i wonder whether there may be a small bug in spark:iterativereduceflatmap.java in the way the parameters are flatten/deflatten or re-injected in the network, but i'm not sure enough to fill a bug report. because my dbn was training fine on 1 thread, but params were wrong with this spark approach. then, i replaced with my own flattening/deflattening of the parameters, and now it's training fine. i've not been able to dig into the original code to spot the issue, if there is any, but well, this is just to let you know in case you hear about that later on... anyway, plz don't loose time for now about that, it's probably just my fault and thanks for these examples !", "i've made a gist with my proposed modifications:  link", "pull out a pr with a test. i'll be happy to merge it"], ["maybe someone can help me\u2026 i need to know what\u2019s happening with dl4j. no one answers here. no one answers on the konduit forum. treo and raver119 are inactive and alexdblack is no longer merging prs. new commits are close to zero. \"support \" is just linking to the examples. is this project even maintained any more? by who? who\u2019s planning to develop it? where are the engineers that wrote the code base? why hasn\u2019t there been an announcement about maintainer departures? need some transparency...", "can you link your forum post here?  i agree, this gitter is great for looking stuff up but it should auto reply the new forum if gitter has that option", "are you associated with the project? do you know the answers to the questions i just asked?", "i didn't say this gitter is great. it's not very active actually, compared to others. that's what i'm asking about. it does auto-reply the new forum, which is also not very active. where are the engineers that know the code and used to support developers?", "here are the top committers:  link . how many among the top 6 or 10 are still making prs? i don't see them any more. that's why i'm asking.", "i'm not associated with project. i did a search and found out that there is funding for a company called skymind and i believe they are the \"custodians\" or something like that. so they really only respond to issues or tough questions. gitter sucks imo. but as a resource you can search back and find answers to most questions...so thats the great part. youll also notice that people ask the same questions over and over. ive only been using it for 4 months but if you have a question ill try and point you in the right direction", "i really appreciate that. thanks.can you point me to the skymind website? i don't see it for some reason... i'm trying to justify to my team adopting dl4j so this is research. but i'm concerned about support. update: looks like konduit is the custodian, from the dl4j repo. can't tell if they're funded.", "link", "cool. but the link on techcrunch redirects to a company called pathmind now... anyway is looks like konduit has taken over this project:  link . just look at the bottom of the readme. but there's no commits so it doesn't really matter which company it is", "isnt the most recent commit 12 days ago?", "just look at the graph...  link", "what kind of project do you have...if you dont mind me asking?", "we have a couple. one is topic modeling with nlp. trying to classify a text.", "anyway, until i can figure out where the main dl4j committers went, i'll just assume i need to look at other tools. thank you for your help."], ["there is no example to implement custom mdp in rl4j", "can someone help me with an example implementation of mdp ? i don't want to use gym", "it seems gym had not update for 3 years", "i don't want to use gym but a simple example to use mdp in rl4j.", "i'm building such a simple example using pavlov's dog as \"problem\", you can find it here:  link . it has a cli version and a ui to visually display the training (and soon the using) of the model. it doesn't require anything special (no gym or web server...) only rl4j (through maven) and java.", "link"], ["[error] failed to execute goal class :maven-enforcer-plugin: id :enforce (enforce-default) on project dl4j-examples: some enforcer rules have failed. look above for specific messages explaining why the rule failed. -> [help 1]", "what are you trying to do?", "you shouldn't get that by default. what did you modif?", "i feel \"building from source\" attempt...", "probably", "i see \"dl4j-examples\" though"], ["hi. i think i found a bug in the tfidfvectorizer and would like to confirm before submitting a pr. if i use a labelawareiterator to fit the vectorizer, it always yields a npe when i try to use the vectorize method.", "tfidfvectorizer doesn't pass its labelssource to the labelawareiterator (like it does for the other two types of iterators).  code  that means the labelssource doesn't get populated during fitting and  code  expects labelssource to be non-empty... this results in an npe."], ["i need help to submit spark job with deeplearning4j in cluster mode. it is running fine in client mode, but in cluster mode nothing is happening.", "no errors or logs?", "nothing, in client mode jar is uploading logs are coming, but in cluster mode even jars not getting uploaded , and not logs is coming.", "please help me. is there any way to do the same."], ["hi guys!! i would like to know what model i should use for tweet analysis. i feel confused with the part that all data must be numerical. how do i represent the content of a tweet numerically?"], ["hi, im seeking advice about working in field of ml/ds. im mechatronic engineer and looking to shift my career toward this field. does this field have future for applied tasks. i working since 8 years in field automation (buildings). so, what resources online to start and get an entry level skills to apply for job. is master degree/phd is required to be proficient or could from study online resources?", "start with the book machine learning with action or tune into andrew ng's machine learning playlist and later on start working on hands on machine learning using scikit and tensorflow", "do you think that this filed could be studed online? or should i have master degree?"], ["hi, i'm trying to import a keras (functional) model into dl4j, i'm using 1.beta7 but have also tried previous version, it gives me an error:  code . for more information, see  link  can anyone help? maybe it has something to do with the architecture or something.. i can't figure it out.", "btw, the sequential keras models work fine, yet, the functional one, even the ones from the konduit tutorial, don't work.", "there\u2019s a separate api for functional models.", "jfyi, for search results, i would suggest moving questions over to the forum if possible. a sequential model is a multi layer network, a functional model is a computation graph. there's 2 different method calls for import to use."], ["i am looking at the tsne and word2vec example. the code in tsnestandardexample runs but does not plot anything at the end. the documentation on  link  shows different code, ending in vec.lookuptable().plotvocab(tsne); which does not match what's in tsnestandardexample. how do i get a plot of the result?", "there\u2019s 2 options: either render to uiserver, or save to file as csv"], ["i'm having a problem with nd4jcuda. in particular, my fit loop will randomly get stuck and make no progress. a thread dump shows that the threads seem to be stuck in this method:  code  and  code . and idea what this could be caused by?", "is there maybe an issue with multi threaded nd4j array access and allocation?", "file an !issue please, and post as much details as possible please"], ["i want to solve the conflict, can i load libgomp from jars by modify nd4j's source code? if don't care the system, how to use javacpp load library from specify path before load from directory", "wouldn't setting ld_preload work?", "try building libnd4j from sources?,in worst case you'll just remove few pragmas, probably around declare simd", "javacpp does that by default, something else is loading system libraries", "wether i can remove the gomp from platform.preload in the linux-x86_64-nd4j.properties?", "but yes, like says using ld_library_path and/or ld_preload should force whatever to use what you want", "ok i go to have try, thank you"], ["where can i find resources to understand how dl4j works interally?", "depends what you are looking for... it's a huge projectare you trying to implement a new type of neural network or something?"], ["when producing feature maps for convolution layer from previous pooling layer, to  my understanding, it's a full connection between all feature maps in pooling layer and one in convolution layer. is this the groundtruth? it is according to \"  link  \""], ["hey everybody, i have n csv files called  code .csv in my folder. each of those files contains a timeseries where each line corresponds to one timestamp. i have read that i should use csvsequencerecordreader for this, but i cannot find any examples how to use it. can anybody help with that?", "loading the image as  code  vs  code  now gives the same result as in tf", "i googled for \u201ccsvsequencerecordreader test\u201d and got some examples:  link", "thanks a lot. it worked. does anyone know if there is the ability to shuffle the order in which the datafiles are trained using the numberedfileinputsplit?"], ["i'm unable to get the keras model loaded in deep learning 4j with beta5 on macos catalina giving fatal runtime error in java. the same code works on ubuntu linux with beta5. is this a known issue? if yes, please let me know by when the fix will be available."], ["i learn some examples from named directories, unfortunately i only can see the indexes of the labels but not its names where loaded from, are these persisted in the model somewhere or do i have to care of it in the learning stage when loaded via filesplit?", "that's where the record meta data comes in", "but metadata i know for the datasets during learning, is there also metadata to store within a model for the labels?", "you'd have to track that yourself", "we are working on a more integrated pipeline api that will allow this to be a bit less opaque", "if you have any ideas for features feel free to open an issue"], ["hi, i'm encountering a weird behaviour: the gpu vram usage increases over time when adding an evalutivelistener to my nn. basically, i noticed that each time an evaluation is performed, the basic vram usage is increased (i'm using the uiserver as well as nvidia-smi to check that this is true). i wrote my own datasetiterator (with  code ), which is a wrapper for an underlying iterator, but it is the same used for the training dataset and the validation dataset. i'm using id -beta and running on a 1080 ti. thanks!", "add explicit  code  call at the end of epoch. and tell me what changes after that", "is there a way to easily do that with a listener? (in case there is, just point me to the class!). found it, i'll extend basetraininglistener", "nope, still happening", "file an !issue and post your source code as !gist. or, better, make something i could reproduce locally", "i'll see if i can give you something reproducible, the code i'm working on is proprietary and i cannot share it, i'll try my best"], ["hi, i've been taking a look at some of the dl4j source code and i was wondering if i could get some help getting oriented? in particular i was trying to understand how keras model import works, but i'm not sure i'm looking in the right place. i'm on the release/1.-beta5 branch and looking at  code . the constructor (line 96) is using an  code  object to get the parameters for the model, but it seems to be calling a lot of methods that are not implemented by the builder, such as  code . the kerasmodelbuilder appears to be using a different naming convention for its getters, and not including the word \"get\". so instead of  code  and  code  it has  code  and  code . anyway, right now this looks like it won't even compile - am i missing something obvious?", "for keras import example code have a look here:  link", "thanks for that, but right now i'm more interested in understanding the implementation than the practical application.", "ok, i just took another look and i was missing something obvious. the getters are provided by lombok.data.", "sounds like you are looking in the right place. main classes are kerasmodelbuilder, kerasmodel, and the layers here:  link . for example: this converts the keras convolution layer config to a dl4j convolutionlayer instance:  link . does that clarify things? and yeah, for lombok - you'll probably need a lombok ide plugin for lombok if you're looking at the code in an ide. see \"install\" menu here:  link", "thanks, i've got my environment set up and can compile now."], ["hey @alexdblack. i'm updating wekadeeplearning4j and was wondering when a new release of dl4j will be out. we've added all the efficientnet models but they can't be used as of yet until this pull request is released  issue . we can't use the snapshot as we're using gradle."]], "role": [[0, 0, 1], [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1, 0, 1], [0, 1, 1, 1, 0, 0, 1, 1, 1, 0, 0, 1, 0], [0, 0], [0, 1, 1, 1, 0], [0, 1, 1, 0, 1, 0, 1, 0], [0], [0, 1, 0, 0, 1, 0], [0, 1, 0, 0], [0, 0, 1], [0, 0, 1, 0], [0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 0], [0, 0, 0], [0, 1, 0, 1, 0], [0, 1, 1, 0, 0, 1, 0, 0, 1, 1], [0, 0, 1, 0], [0, 0, 1, 1, 1, 1, 1, 1, 0], [0], [0, 1], [0, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 1, 0, 1, 0, 1, 0, 1], [0, 0], [0], [0, 1, 1, 1], [0, 0, 0, 0], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0], [0, 0, 1, 1, 1, 0], [0], [0, 0], [0, 1, 0, 0, 1, 0], [0, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0], [0], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0], [0, 0, 0, 0], [0, 0], [0, 1, 0, 1, 0], [0, 0, 1, 0, 1, 0], [0, 0, 0, 0], [0, 1], [0, 0, 0], [0, 1, 0], [0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 0], [0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 1, 1, 0], [0, 1, 1, 0, 1, 0, 1], [0, 1, 1, 1, 0, 1, 1, 1, 0, 0], [0, 0], [0, 0], [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0], [0, 1], [0, 1, 1, 0], [0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0], [0, 0, 1], [0, 1], [0], [0, 1, 1, 0], [0, 0, 1, 0, 1, 1, 1, 1, 0], [0, 1, 0], [0, 1, 0, 0, 1, 1, 0], [0], [0, 1], [0], [0, 1, 1, 1, 0, 1, 1, 0], [0, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1], [0, 0, 0, 1, 1, 1], [0, 0], [0, 1], [0, 1, 0, 1, 1, 0, 1], [0, 1, 0, 1, 0, 0], [0, 0], [0, 1, 0, 1, 1], [0, 1, 0, 1, 1, 0], [0, 1, 0, 1, 0, 0], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1], [0, 1, 0, 1, 0, 1, 1, 0, 0, 0], [0, 1, 0], [0, 0, 0, 1], [0, 0], [0], [0], [0, 0, 1, 0, 1, 0], [0, 0, 1], [0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1], [0, 0, 1, 1, 0], [0, 0, 1, 1, 0], [0, 0, 1, 1, 0], [0, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 1, 0], [0, 1], [0, 0, 1, 0], [0, 1, 1, 0, 0, 1, 0, 0], [0, 1, 1, 0, 1, 0], [0, 1, 0], [0, 0, 0, 1], [0, 1, 0, 1, 0, 1, 0], [0, 0], [0, 0, 1, 0, 1, 0], [0], [0, 1], [0, 1, 0, 1, 0, 1, 0], [0, 1], [0, 0], [0, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0], [0, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1], [0, 1, 1, 0, 1, 1, 1, 0, 1, 0], [0, 1, 0], [0, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0], [0, 1, 0], [0, 1, 1], [0, 1, 0, 1, 0], [0, 1], [0, 1, 1, 1, 1, 0, 0], [0, 1, 0], [0, 1, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 1], [0, 1, 0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 0], [0, 0, 1, 0, 1, 0], [0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0, 0, 1], [0, 1, 0], [0, 1], [0], [0, 1, 1, 0, 1, 0, 0, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0], [0, 1, 0, 1], [0, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1], [0, 1, 0, 0, 1, 0], [0], [0, 1, 0], [0, 1, 0, 0, 1], [0, 0], [0, 1, 0, 1, 0, 1], [0, 0], [0, 1, 0, 1, 0, 1], [0, 1, 1, 1, 1, 0, 0, 1, 1, 0], [0, 1, 0, 0, 1, 0, 1, 0, 0, 1], [0, 1, 1], [0, 1, 1, 0, 1], [0, 1, 1], [0, 1, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0], [0], [0, 1], [0, 1, 1], [0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 0], [0], [0, 1, 1, 0, 0, 1, 0, 0], [0, 1, 1], [0, 0, 0], [0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 1, 0, 1], [0, 1, 0, 1, 0, 1, 0, 1], [0, 0, 0, 1, 0, 1, 0, 1], [0, 1, 1], [0, 0, 0, 1, 0, 1], [0], [0, 1, 0, 0], [0, 1, 1, 1, 0, 1, 0, 1, 0, 1, 0, 0, 1, 0], [0, 0, 0], [0, 1, 1, 0, 0, 1, 1, 1], [0, 1, 1, 1, 0], [0, 1], [0], [0, 0], [0, 1], [0, 1, 1, 1, 0, 1], [0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0], [0, 0], [0, 1, 0, 1, 0, 1, 1], [0], [0, 0, 0], [0, 1, 0, 1, 1, 0, 1], [0, 1, 1, 0, 1], [0, 0, 1], [0, 0, 1, 0, 1, 0, 1], [0, 1, 0], [0, 1, 0, 0, 1, 0, 1], [0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1], [0, 1, 0, 1], [0, 1, 0], [0, 1, 0, 1, 0], [0, 1, 1, 1, 1, 0, 1, 1, 1, 0, 1, 1, 1, 1, 0], [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 1], [0, 1, 0, 1, 0, 1], [0, 0, 1, 0], [0, 1, 0], [0, 1], [0, 1, 1, 0, 1, 1, 0, 1, 1, 0, 0], [0, 1, 0, 0, 1, 1, 0], [0, 1, 0], [0, 1, 0, 1], [0, 1, 0], [0, 0], [0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1], [0], [0, 0, 1, 0], [0], [0, 1, 1, 1, 0, 0, 1, 0, 0], [0, 0, 1, 1, 0, 0, 1], [0, 1, 0, 0], [0, 1, 0, 1, 1, 0], [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0], [0], [0, 0, 1, 0], [0, 1, 1, 1, 1, 0, 1, 0, 0], [0, 0, 1], [0, 1, 0, 0], [0, 0, 1, 1], [0, 1, 0, 1, 0, 1, 0, 0, 1, 0, 1], [0, 1, 0, 0, 0, 1], [0, 1, 0], [0, 1, 1, 0, 0, 0, 0, 0], [0], [0, 1, 0], [0, 1, 0, 0, 0], [0, 1, 0, 1], [0, 1], [0, 1, 1, 0, 1, 0, 1, 1, 0], [0, 1, 1, 1, 1, 0, 0, 1, 1], [0, 0, 0], [0, 1, 0], [0, 1], [0, 0, 1, 1, 1, 1, 1, 0, 1, 1, 0], [0], [0, 0], [0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 1, 0], [0], [0, 0, 1], [0, 0, 1], [0, 1, 0, 0, 0, 1, 0, 1, 0, 1, 0, 1, 0, 0], [0, 0, 1, 0, 1, 1], [0, 1, 1, 1, 1, 1], [0, 0], [0, 1, 0, 0], [0], [0, 1, 0], [0, 0, 1, 1], [0, 1], [0, 0, 1], [0, 1, 1, 1, 0, 1, 0], [0, 1], [0], [0, 1, 1, 0], [0], [0, 1, 0, 1, 1, 1], [0, 1, 0, 0, 1, 1], [0, 1, 0, 0, 1, 0], [0]], "edge": [[[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [6, 8], [8, 6], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [3, 5], [5, 3], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [0, 4], [4, 0], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [3, 4], [4, 3], [0, 5], [5, 0], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [7, 9], [9, 7]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 0]], [[0, 1], [1, 0], [0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [3, 4], [4, 3], [5, 6], [6, 5]], [[0, 1], [1, 0], [0, 2], [2, 0], [1, 3], [3, 1], [2, 4], [4, 2], [4, 5], [5, 4], [3, 6], [6, 3], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [11, 13], [13, 11], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16], [17, 18], [18, 17], [18, 19], [19, 18], [19, 20], [20, 19], [20, 21], [21, 20], [21, 22], [22, 21], [22, 23], [23, 22], [23, 24], [24, 23], [24, 25], [25, 24], [25, 26], [26, 25], [26, 27], [27, 26]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [4, 7], [7, 4], [7, 8], [8, 7], [6, 9], [9, 6]], [[0, 1], [1, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [0, 2], [2, 0], [1, 3], [3, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [0, 4], [4, 0], [2, 5], [5, 2], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 0]], [[0, 1], [1, 0]], [[0, 0]], [[4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2], [4, 5], [5, 4]], [[0, 1], [1, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [3, 5], [5, 3]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0]], [[0, 0]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [0, 5], [5, 0], [5, 6], [6, 5], [6, 7], [7, 6], [0, 8], [8, 0], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [2, 4], [4, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 0]], [[0, 1], [1, 0], [0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16], [17, 18], [18, 17], [18, 19], [19, 18], [19, 20], [20, 19], [20, 21], [21, 20], [21, 22], [22, 21], [22, 23], [23, 22], [23, 24], [24, 23], [24, 25], [25, 24], [25, 26], [26, 25]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [3, 5], [5, 3], [4, 6], [6, 4], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16], [17, 18], [18, 17], [18, 19], [19, 18], [19, 20], [20, 19], [20, 21], [21, 20], [19, 22], [22, 19], [19, 23], [23, 19], [19, 24], [24, 19], [24, 25], [25, 24], [25, 26], [26, 25], [26, 27], [27, 26], [27, 28], [28, 27], [28, 29], [29, 28]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0], [0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [6, 11], [11, 6], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [4, 13], [13, 4], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [2, 4], [4, 2], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8]], [[0, 1], [1, 0], [0, 2], [2, 0], [1, 3], [3, 1], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [0, 2], [2, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [3, 5], [5, 3], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15]], [[0, 0]], [[0, 1], [1, 0], [0, 2], [2, 0], [1, 3], [3, 1], [2, 4], [4, 2], [0, 5], [5, 0], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[1, 2], [2, 1], [1, 3], [3, 1], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [0, 2], [2, 0], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15], [16, 17], [17, 16]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [2, 4], [4, 2], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[1, 2], [2, 1], [1, 3], [3, 1], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14], [15, 16], [16, 15]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [2, 4], [4, 2], [3, 5], [5, 3], [5, 6], [6, 5], [6, 7], [7, 6], [4, 8], [8, 4], [7, 9], [9, 7], [9, 10], [10, 9], [10, 11], [11, 10], [9, 12], [12, 9], [12, 13], [13, 12], [13, 14], [14, 13]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [0, 4], [4, 0], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [2, 4], [4, 2], [4, 5], [5, 4], [5, 6], [6, 5], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [2, 4], [4, 2], [3, 5], [5, 3], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [0, 2], [2, 0], [0, 4], [4, 0], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 3], [3, 1], [2, 4], [4, 2], [3, 5], [5, 3], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [1, 6], [6, 1], [6, 7], [7, 6], [7, 8], [8, 7]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9]], [[0, 0]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [0, 3], [3, 0], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12], [13, 14], [14, 13], [14, 15], [15, 14]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5], [6, 7], [7, 6], [7, 8], [8, 7], [8, 9], [9, 8], [9, 10], [10, 9], [10, 11], [11, 10], [11, 12], [12, 11], [12, 13], [13, 12]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [1, 4], [4, 1], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [0, 3], [3, 0], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 1], [1, 0]], [[0, 1], [1, 0], [1, 2], [2, 1]], [[0, 1], [1, 0], [0, 2], [2, 0], [0, 3], [3, 0], [3, 4], [4, 3], [4, 5], [5, 4], [5, 6], [6, 5]], [[0, 1], [1, 0]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2]], [[0, 0]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [1, 3], [3, 1], [3, 4], [4, 3], [4, 5], [5, 4]], [[0, 1], [1, 0], [1, 2], [2, 1], [2, 3], [3, 2], [0, 4], [4, 0], [4, 5], [5, 4]], [[0, 0]]], "label": [1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 1, 0, 1]}